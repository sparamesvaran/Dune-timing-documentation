{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the Dune Timing documentation website. This area can be used as a collaborative working area for documentation that has not yet been finalized. Once the task force has agreed to adopt a document, it should be placed in the repository, and the links here should be updated accordingly. First steps","title":"Home"},{"location":"#first-steps","text":"","title":"First steps"},{"location":"Developers-Guide/","text":"Developer's Guide Contributing to the Application Framework During the prototyping stage for the Application Framework, it is recommended to clone the repository and make your changes in a branch. The branch name should be of the form user/ShortDescription for clarity in the code browser. This option allows other users to base their changes off of yours, if needed, in a more transparent way than forking. A code linting tool has been developed by John Freeman that helps partially ensure that code is compliant with our style guide, though many potential guideline violations are impossible for a script (as opposed to a human) to catch. Details are below. Once your changes are stable, a pull request should be initiated. Pull requests follow this procedure: 1. The code will be reviewed during an Application Framework meeting 1. Comments related to the pull request's intended changes should be incorporated into the branch 1. Other comments that may arise should be captured as Github issues 1. The pull request will be allowed to sit for a few days to make sure that there are no further comments 1. Once all comments have been resolved, the Application Framework managers will meet and merge the pull request Using the C++ code linter, dune-cpp-style-check.sh Before code is merged into the develop branch, it should be in conformance with the DUNE C++ Style Guide, available here . As mentioned above, a subset of the guidelines are capable of being automatically checked for using a linter tool, dune-cpp-style-check.sh . Obtaining dune-cpp-style-check.sh is straightforward, as it resides in the same repository, \"styleguide\", as the actual text of the style guide: git clone https://github.com/DUNE-DAQ/styleguide/ In the instructions which follow, it's assumed you've installed the styleguide repo in $STYLEGUIDE_DIR, and the location of your build output is $BUILD_DIR. First, please read the instructions on how to use the linter, which are printed to output when you run it without arguments: $STYLEGUIDE_DIR/cpplint/dune-cpp-style-check.sh Then, keeping in mind that the most likely location of the JSON compile_commands.json file is in the build directory, to run the linter on a file do the following: $STYLEGUIDE_DIR/cpplint/dune-cpp-style-check.sh $BUILD_DIR <name of file> Be aware that the linter simply doesn't have the capability to catch some violations of the guidelines (e.g., figuring out that you've declared data protected instead of private in a class, or that you've written a function that's meant to do a bunch of unrelated things), and is no substitute for having read and understood the guide.","title":"Developer's Guide"},{"location":"Developers-Guide/#developers-guide","text":"","title":"Developer's Guide"},{"location":"Developers-Guide/#contributing-to-the-application-framework","text":"During the prototyping stage for the Application Framework, it is recommended to clone the repository and make your changes in a branch. The branch name should be of the form user/ShortDescription for clarity in the code browser. This option allows other users to base their changes off of yours, if needed, in a more transparent way than forking. A code linting tool has been developed by John Freeman that helps partially ensure that code is compliant with our style guide, though many potential guideline violations are impossible for a script (as opposed to a human) to catch. Details are below. Once your changes are stable, a pull request should be initiated. Pull requests follow this procedure: 1. The code will be reviewed during an Application Framework meeting 1. Comments related to the pull request's intended changes should be incorporated into the branch 1. Other comments that may arise should be captured as Github issues 1. The pull request will be allowed to sit for a few days to make sure that there are no further comments 1. Once all comments have been resolved, the Application Framework managers will meet and merge the pull request","title":"Contributing to the Application Framework"},{"location":"Developers-Guide/#using-the-c-code-linter-dune-cpp-style-checksh","text":"Before code is merged into the develop branch, it should be in conformance with the DUNE C++ Style Guide, available here . As mentioned above, a subset of the guidelines are capable of being automatically checked for using a linter tool, dune-cpp-style-check.sh . Obtaining dune-cpp-style-check.sh is straightforward, as it resides in the same repository, \"styleguide\", as the actual text of the style guide: git clone https://github.com/DUNE-DAQ/styleguide/ In the instructions which follow, it's assumed you've installed the styleguide repo in $STYLEGUIDE_DIR, and the location of your build output is $BUILD_DIR. First, please read the instructions on how to use the linter, which are printed to output when you run it without arguments: $STYLEGUIDE_DIR/cpplint/dune-cpp-style-check.sh Then, keeping in mind that the most likely location of the JSON compile_commands.json file is in the build directory, to run the linter on a file do the following: $STYLEGUIDE_DIR/cpplint/dune-cpp-style-check.sh $BUILD_DIR <name of file> Be aware that the linter simply doesn't have the capability to catch some violations of the guidelines (e.g., figuring out that you've declared data protected instead of private in a class, or that you've written a function that's meant to do a bunch of unrelated things), and is no substitute for having read and understood the guide.","title":"Using the C++ code linter, dune-cpp-style-check.sh"},{"location":"FMCtesting/","text":"Here is a list of documents about different aspects the code style we would like to adopt for the DUNE DAQ software. These links points to documents developed by the team on platforms outside this Wiki. DUNE C++ Style guide Code organization CMake coding style Doxygen style C++ formatting Other useful links Google C++ style guide (style guide off of which DUNE's style guide was forked) C++ core guidelines","title":"FMCtesting"},{"location":"FMCtesting/#other-useful-links","text":"Google C++ style guide (style guide off of which DUNE's style guide was forked) C++ core guidelines","title":"Other useful links"},{"location":"Glossary-of-Terms/","text":"Glossary of Terms This page defines the current language being used to describe elements of the DAQ Application Framework. Core Terms or General Concepts DAQ System : A set of DAQ Applications working in concert to perform the functions needed to effect data acquisition. DAQ Application : A single unit of the DAQ System in the form of an executable program that is started and controlled through the CCM. DAQ Applications perform a defined set of tasks based on their configuration. Interface : A pure abstract base class. Plugin : A piece of code provided either by the toolkit or by users (generally-useful plugins should be incorporated into the toolkit if possible), characterized by a specific interface, to perform a single task. Multiple plugins with different implementations can exist for a single interface. Users of plugins should be shielded from any implementation details. The plugin compiled code should be a shared library. Data Flow Programming : (DFP) a paradigm to describe a larger system as a graph in terms of data transformation nodes and data transportation edges DFP Node : a unit of code with an identifier (name) unique in the system with zero or more DFP Ports DFP Port : an identified point on a DFP Node which may produce or accept DFP data message (or both). It typically as a Socket DFP Edge : a data transport conduit between two DFP Ports Socket : the endpoints of a DFP Edge , each associated with a DFP Port . An application may send() a message to a Socket and it may recv() a message from a Socket (eg: BSD sockets, ZeroMQ sockets) Transport : act of transmitting a message between a Socket send() to another Socket recv() , transports include transmission over TCP/IP, Unix domain sockets (\"FIFO\"s) and inter-thread via shared memory. Implementation definitions DAQModule : A distinct set of code which performs a specific task. Implemented by users, will be loaded into a DAQ Application according to the application configuration. Can load plugins. Connected to other User Modules within a DAQ Application via queues. DAQProcess : A central class which will be called from a program's main function, responsible for loading core services and reading the Module List for that program. Every DAQProcess has a unique name and it is spawned by the CCM. GraphConstructor : A class which is responsible for loading the set of DAQModules and Queues needed for a given DAQ Application, as well as linking them together. It returns the loaded modules and buffers to the DAQProcess Component an implementation of one or more Interfaces . Components shall be instantiated via a Factory mechanism,via their component type name and an instance name . Factory a component with restricted construct-ability that provides instantiated Components . Component Type Name string data (not code) identifying a Component type, may be the C++ type name or another. Component Instance Name string data identifying one instantiation of a component. Queue : A class which moves data between User Modules within a DAQ Application and provides methods for storage and retrieval. May provide a queue-like interface, random access or other functionality but at the moment only a basic queue functionality is implemented. Every queue implementation needs to be thread safe.","title":"Glossary of Terms"},{"location":"Glossary-of-Terms/#glossary-of-terms","text":"This page defines the current language being used to describe elements of the DAQ Application Framework.","title":"Glossary of Terms"},{"location":"Glossary-of-Terms/#core-terms-or-general-concepts","text":"DAQ System : A set of DAQ Applications working in concert to perform the functions needed to effect data acquisition. DAQ Application : A single unit of the DAQ System in the form of an executable program that is started and controlled through the CCM. DAQ Applications perform a defined set of tasks based on their configuration. Interface : A pure abstract base class. Plugin : A piece of code provided either by the toolkit or by users (generally-useful plugins should be incorporated into the toolkit if possible), characterized by a specific interface, to perform a single task. Multiple plugins with different implementations can exist for a single interface. Users of plugins should be shielded from any implementation details. The plugin compiled code should be a shared library. Data Flow Programming : (DFP) a paradigm to describe a larger system as a graph in terms of data transformation nodes and data transportation edges DFP Node : a unit of code with an identifier (name) unique in the system with zero or more DFP Ports DFP Port : an identified point on a DFP Node which may produce or accept DFP data message (or both). It typically as a Socket DFP Edge : a data transport conduit between two DFP Ports Socket : the endpoints of a DFP Edge , each associated with a DFP Port . An application may send() a message to a Socket and it may recv() a message from a Socket (eg: BSD sockets, ZeroMQ sockets) Transport : act of transmitting a message between a Socket send() to another Socket recv() , transports include transmission over TCP/IP, Unix domain sockets (\"FIFO\"s) and inter-thread via shared memory.","title":"Core Terms or General Concepts"},{"location":"Glossary-of-Terms/#implementation-definitions","text":"DAQModule : A distinct set of code which performs a specific task. Implemented by users, will be loaded into a DAQ Application according to the application configuration. Can load plugins. Connected to other User Modules within a DAQ Application via queues. DAQProcess : A central class which will be called from a program's main function, responsible for loading core services and reading the Module List for that program. Every DAQProcess has a unique name and it is spawned by the CCM. GraphConstructor : A class which is responsible for loading the set of DAQModules and Queues needed for a given DAQ Application, as well as linking them together. It returns the loaded modules and buffers to the DAQProcess Component an implementation of one or more Interfaces . Components shall be instantiated via a Factory mechanism,via their component type name and an instance name . Factory a component with restricted construct-ability that provides instantiated Components . Component Type Name string data (not code) identifying a Component type, may be the C++ type name or another. Component Instance Name string data identifying one instantiation of a component. Queue : A class which moves data between User Modules within a DAQ Application and provides methods for storage and retrieval. May provide a queue-like interface, random access or other functionality but at the moment only a basic queue functionality is implemented. Every queue implementation needs to be thread safe.","title":"Implementation definitions"},{"location":"versions/devel/Compiling-and-running/","text":"To get set up, you'll need access to the ups product area /cvmfs/dune.opensciencegrid.org/dunedaq/DUNE/products , as is the case, e.g., on the lxplus machines at CERN. If you're on a system which has access to this product area, simply do the following after you've logged in to your system and created an empty directory (we'll refer to it as \"MyTopDir\" on this wiki): curl -O https://raw.githubusercontent.com/DUNE-DAQ/daq-buildtools/develop/bin/quick-start.sh chmod +x quick-start.sh ./quick-start.sh which will install the needed repos and provide you with a file you can then source to set up the build environment, setup_build_environment , and an executable script you can run for builds, build_daq_software.sh . If, after running quick-start.sh , you want to build the appfwk package and install it in a local subdirectory called ./install right away, all you need to do is the following: . ./setup_build_environment # Only needs to be done once in a given shell ./build_daq_software.sh --install build_daq_software.sh will by default skip CMake's config+generate stages and go straight to the build stage unless the CMakeCache.txt file isn't found in ./build/appfwk -- which is of course the case the first time you run it. If you want to remove all the contents of ./build/appfwk and run config+generate+build, all you need to do is add the --clean option, i.e. ./build_daq_software.sh --clean --install And if, after the build, you want to run the unit tests, just add the --unittest option. Note that it can be used with or without --clean , so, e.g.: ./build_daq_software.sh --clean --install --unittest # Blow away the contents of ./build/appfwk, run config+generate+build, and then run the unit tests ..where in the above case, you blow away the contents of ./build/appfwk, run config+generate+build, install the result in ./install and then run the unit tests. If you want to develop a package other than appfwk, you'll want to add the --pkgname <package name> option. E.g., if we want to build listrev (a simple DAQModule package written by Kurt Biery and documented elsewhere on this wiki), from MyTopDir we can do the following: git clone https://github.com/DUNE-DAQ/listrev.git ./build_daq_software.sh --install --pkgname listrev ...where here it's assumed you've already built and locally installed appfwk since it's a dependency of listrev. If you want to see verbose output from the compiler, all you need to do is add the --verbose option: ./build_daq_software.sh --verbose --pkgname <package whose build you're troubleshooting> To check for deviations from the coding rules described in the DUNE C++ Style Guide , run with the --lint option: ./build_daq_software.sh --lint ...though be aware that some guideline violations (e.g., having a function which tries to do unrelated things) can't be picked up by the automated linter. You can see all the options listed if you run the script with the --help command, i.e. ./build_daq_software.sh --help Finally, note that both the output of your builds and your unit tests are logged to files in the ./log subdirectory. These files may have ASCII color codes which make them difficult to read with some tools; more or cat , however, will display the colors and not the codes themselves. Running In order to run the applications built during the above procedure, the system needs to be instructed on where to look for the libraries that can be used to instantiate objects. In order to do that, the environmental variable CET_PLUGIN_PATH has to contain the path to your compiled src and test sub-directories. This is handled by the ./setup_runtime_environment script which was placed in MyTopDir when you ran quick-start.sh; all you need to do is source it: . ./setup_runtime_environment Note that if you add a new repo to your development area, after building your new code you'll need to source the script again. Once the runtime environment is set, just run the application you need. For example, from MyTopDir, you can run daq_application -c QueryResponseCommandFacility -j appfwk/test/producer_consumer_dynamic_test.json This example accepts commands configure , start , stop , unconfigure , quit . daq_application Command Line Arguments Use `daq_application --help` to see all of the possible options: $ ./build/appfwk/apps/daq_application --help ./build/appfwk/apps/daq_application known arguments (additional arguments will be stored and passed on): -c [ --commandFacility ] arg CommandFacility plugin name -m [ --configManager ] arg ConfigurationManager plugin name -s [ --service ] arg Service plugin(s) to load -j [ --configJson ] arg JSON Application configuration file name -h [ --help ] produce help message Some additional information ### TRACE Messages To enable the sending of TRACE messages to a memory buffer, you can set one of several TRACE environmental variables _before_ running `appfwk/apps/simple_test_app`. One example is to use a command like `export TRACE_NAME=TRACE`. (For more details, please see the [TRACE package documentation](https://cdcvs.fnal.gov/redmine/projects/trace/wiki/Wiki). For example, the [Circular Memory Buffer](https://cdcvs.fnal.gov/redmine/projects/trace/wiki/Circular_Memory_Buffer) section in the TRACE Quick Start talks about the env vars that you can use to enable tracing.) To view the TRACE messages in the memory buffer, you can use the following additional steps: * [if not done already] `export SPACK_ROOT= ; source $SPACK_ROOT/setup-env.sh` * [if not done already] `spack load trace` * `trace_cntl show` or `trace_cntl show | trace_delta -ct 1` (The latter displays the timestamps in human-readable format. Note that the messages are listed in reverse chronological order in both cases.) Testing and running inside a docker container Requirements on the host machine: * installation of cvmfs (instructions can be found here ); * installation of docker (instructions are available on its official website). Docker image recommended to use is dingpf/artdaq:latest . Its corresponding dockerfile can be found in this repo . Use docker run --rm -it -v /cvmfs:/cvmfs -v $PWD:/scratch dingpf/artdaq to start the container, where it mounts your local working directory as /scratch inside the container. You can then treat /scratch as the empty MyTopDir directory referred to in the instructions at the top of this page. Next Steps [[Creating a new package|Instructions-for-creating-a-new-package ]] [[Step-by-step instructions for creating your own DAQModule package|Step-by-step-instructions-for-creating-your-own-DAQModule-package]]","title":"Compiling and running"},{"location":"versions/devel/Compiling-and-running/#running","text":"In order to run the applications built during the above procedure, the system needs to be instructed on where to look for the libraries that can be used to instantiate objects. In order to do that, the environmental variable CET_PLUGIN_PATH has to contain the path to your compiled src and test sub-directories. This is handled by the ./setup_runtime_environment script which was placed in MyTopDir when you ran quick-start.sh; all you need to do is source it: . ./setup_runtime_environment Note that if you add a new repo to your development area, after building your new code you'll need to source the script again. Once the runtime environment is set, just run the application you need. For example, from MyTopDir, you can run daq_application -c QueryResponseCommandFacility -j appfwk/test/producer_consumer_dynamic_test.json This example accepts commands configure , start , stop , unconfigure , quit . daq_application Command Line Arguments Use `daq_application --help` to see all of the possible options: $ ./build/appfwk/apps/daq_application --help ./build/appfwk/apps/daq_application known arguments (additional arguments will be stored and passed on): -c [ --commandFacility ] arg CommandFacility plugin name -m [ --configManager ] arg ConfigurationManager plugin name -s [ --service ] arg Service plugin(s) to load -j [ --configJson ] arg JSON Application configuration file name -h [ --help ] produce help message Some additional information ### TRACE Messages To enable the sending of TRACE messages to a memory buffer, you can set one of several TRACE environmental variables _before_ running `appfwk/apps/simple_test_app`. One example is to use a command like `export TRACE_NAME=TRACE`. (For more details, please see the [TRACE package documentation](https://cdcvs.fnal.gov/redmine/projects/trace/wiki/Wiki). For example, the [Circular Memory Buffer](https://cdcvs.fnal.gov/redmine/projects/trace/wiki/Circular_Memory_Buffer) section in the TRACE Quick Start talks about the env vars that you can use to enable tracing.) To view the TRACE messages in the memory buffer, you can use the following additional steps: * [if not done already] `export SPACK_ROOT= ; source $SPACK_ROOT/setup-env.sh` * [if not done already] `spack load trace` * `trace_cntl show` or `trace_cntl show | trace_delta -ct 1` (The latter displays the timestamps in human-readable format. Note that the messages are listed in reverse chronological order in both cases.)","title":"Running"},{"location":"versions/devel/Compiling-and-running/#testing-and-running-inside-a-docker-container","text":"Requirements on the host machine: * installation of cvmfs (instructions can be found here ); * installation of docker (instructions are available on its official website). Docker image recommended to use is dingpf/artdaq:latest . Its corresponding dockerfile can be found in this repo . Use docker run --rm -it -v /cvmfs:/cvmfs -v $PWD:/scratch dingpf/artdaq to start the container, where it mounts your local working directory as /scratch inside the container. You can then treat /scratch as the empty MyTopDir directory referred to in the instructions at the top of this page.","title":"Testing and running inside a docker container"},{"location":"versions/devel/Compiling-and-running/#next-steps","text":"[[Creating a new package|Instructions-for-creating-a-new-package ]] [[Step-by-step instructions for creating your own DAQModule package|Step-by-step-instructions-for-creating-your-own-DAQModule-package]]","title":"Next Steps"},{"location":"versions/devel/Creating-a-new-package/","text":"Setting up a development area To create a new package, you'll want to install a DUNE-DAQ development environment and then create a new CMake project for the package. How to install and build the DUNE-DAQ development environment is described here . By the time you've set up the development environment as described in the linked instructions, you'll have already downloaded, built, and locally installed the CMake project for appfwk. You'll likely also have noticed that ./appfwk is a git repo; you'll want your project to reside in a git repo as well. A package's subdirectory structure To learn a bit more about how to structure your package so that it can be incorporated into the DUNE DAQ software suite, we'll play with a contrived package called \"toylibrary\". It's actually contained within a subdirectory of the daq-buildtools repo; however, in order to be able to build toylibrary we'll want to copy it into the base directory of the development area. Assuming you're already in the base directory, just do cp -rp daq-buildtools/toylibrary . You can now build it using the normal commands: . ./setup_build_environment ./build_daq_software.sh --install --pkgname toylibrary In terms of its actual functionality, it's pretty useless (it contains a class which can wrap an integer, and another class which can print that wrapped integer). However, its functionality is beside the point; toylibrary contains many features which DUNE DAQ packages have in common, in particular DUNE DAQ packages which provide a libraries other developers want to link against. In fact, if you run ls appfwk and ls toylibrary , you'll notice many subdirectories in common. These include: src : contains the source files meant to be built into the package's shared object library/libraries include : contains the headers users of your package should #include unittest : contains the unit tests you write to ensure that your individual classes, functions, etc. behave as expected test : contains any applications you've written for the purpose of integration testing - ensuring that your software components interact as expected If your package contains applications intended not for testing but for the end user, you'd put the code for it in a subdirectory called apps . toylibrary doesn't have this type of application, but the appfwk package does. Coding rules Along with having a standard directory structure, the C++ code itself in toylibrary conforms to the DUNE C++ Style Guide . Here, \"style\" doesn't mean whitespace and formatting, but rather, a set of Modern C++ best practices designed to make your code more robust against bugs, easier to extend, easier to reuse, etc. The DUNE C++ Style Guide is derived from the Google C++ Style Guide, but is greatly simplified and has been modified to be more appropriate to the DUNE DAQ project than Google's projects. Code which is merged into a package's git develop branch should be in conformance with the guide; while it's encouraged for code on a package's unmerged feature branches to also be in conformance, this is less important. Your project's CMakeLists.txt file Every DUNE DAQ package should have one and only one CMakeLists.txt file, in the base directory of the package (not to be confused with the base directory of the overall development area). To learn a bit about what that CMakeLists.txt file should look like, let's take a look at ./toylibrary/CMakeLists.txt . Because CMake is widely used and extensively documented online, this documentation will primarily focus on DUNE-specific CMake functions. Before doing anything else, we want to define the minimum version of CMake used (currently 3.12, which supports modern CMake style ) as well as the name and version of the project. Concerning the version: it may not literally be the case that the code you're working with is exactly the same as the version-in-question's release code, because you may be on a feature branch, or there may have been commits to the develop branch since the last release. cmake_minimum_required(VERSION 3.12) project(toylibrary VERSION 1.1.0) Next, we want to make CMake functions written specifically for DUNE DAQ development available. Near the top of the file, you'll see the following: set(CMAKE_MODULE_PATH ${CMAKE_CURRENT_SOURCE_DIR}/../daq-buildtools/cmake ${CMAKE_MODULE_PATH}) include(DAQ) This is currently (Aug-5-2020) how we ensure that the CMakeLists.txt file has access to the standard DUNE DAQ CMake functions. It imports the DAQ CMake module which is located in the daq-buildtools repo that's downloaded when you ran quick-start.sh. Note that by convention all functions/macros within the module begin with daq_ , so as to distinguish them from functions/macros from CMake modules written outside of DUNE DAQ. The next step is to call a macro from the DAQ module which sets up a standard DUNE CMake environment for your CMakeLists.txt file: daq_setup_environment() To get specific, daq_setup_environment() will do the following: * Enforce the use of standard, extension-free C++17 * Ensure libraries are built as shared, rather than static * Ensure all code within the project can find the project's public headers * Allow our linter scripts to work with the code * Tell the CMake find_package function to look in the local ./install subdirectory of the development area for packages (e.g., appfwk) * Have gcc use standard warnings * Support the use of CTest for the unit tests Next you'll see calls to CMake's find_package function, which makes toylibrary's dependencies available. Comments in the file explain why the dependencies are selected. Then, you'll see a call to a function called daq_point_build_to . CMake commands which are specific to the code in a particular subdirectory tree of your project should be clustered in the same location of the the CMakeLists.txt file, and they should be prefaced with the name of the subdirectory wrapped in the daq_point_build_to CMake function. E.g., for the code in ./toylibrary/src we have: daq_point_build_to( src ) add_library( toylibrary src/IntPrinter.cpp ) Here, daq_point_build_to(src) will ensure that the ensuing target(s) -- here, the toylibrary shared object file -- will be placed in a subdirectory of ./build/toylibrary directory (build directory) whose path mirrors the path of the source itself. To get concrete, this command means that after a successful build we'll have an executable, ./build/toylibrary/src/libtoylibrary.so , where that path is given relative to the base of your entire development area. Without this function call it would end up as ./build/toylibrary/libtoylibrary.so . With a small project such as toylibrary the benefit may not be obvious, but it becomes more so when many libraries, applications, unit tests, etc. are built within a project. Another function currently provided by the DAQ CMake module is daq_add_unit_test . Examples of this function's use can be found at the bottom of the toylibrary/CMakeLists.txt file, e.g.: daq_add_unit_test(ValueWrapper_test) If you pass this function a name, e.g., MyComponent_test , it will create a unit test executable off of a source file called <your packagename>/unittest/MyComponent_test.cxx , and handle linking in the Boost unit test dependencies. You can also optionally have it link in other libraries by providing them as arguments; in the above example, this isn't needed because ValueWrapper is a template class which is instantiated within the unit test code itself. You can see this linking, however, in ./appfwk/CMakeLists.txt . At the bottom of CMakeLists.txt, you'll see the following function: daq_install(TARGETS toylibrary) The signature of this function is hopefully self-explanatory; basically when you call it it will install the targets (executables, shared object libraries) you wish to make available to others who want to use your package in a directory called ./install/<pkgname> (here, of course, that would be ./install/toylibrary ). You'll also need to add a special file to your project for this function to work; this is discussed more fully in the \"Installing your project as a local package\" section later in this document. Some general comments about CMake While a lot can be learned from looking at the CMakeLists.txt file and reading this documentation, realize that CMake is a full-blown programming language, and like any programming language, the more familiar you are with it the easier life will be. It's extensively documented online; in particular, if you want to learn more about specific CMake functions you can go here for a reference guide of CMake commands, or, if you've sourced setup_build_environment (so CMake is set up), you can even learn at the command line via cmake --help-command <name of command> When you write code in CMake good software development principles generally apply. Writing your own macros and functions you can live up to the Don't Repeat Yourself principle ; an example of this can be found in the definition of the MakeDataTypeLibraries macro in ./appfwk/CMakeLists.txt , which helps avoid boilerplate while instantiating appfwk's FanOutDAQModule template class for various types and giving these instantiations well-motivated names. Another thing to be aware of: since you're using a single CMakeLists.txt file, the scope of your decisions can extend to all ensuing code. So, e.g., if you add the line link_libraries( bloated_library_with_extremely_common_symbol_names ) then all ensuing targets from add_executable , add_library , etc., will get that linked in. For this reason, prefer a target-specific call such as target_link_libraries(executable_that_really_needs_this_library bloated_library_with_extremely_common_symbol_names) instead. In modern CMake it's considered a best practice to avoid functions which globally link in libraries or include directories, and to make things target-specific instead. If your package relies on nonstandard dependencies As you've discovered from the Compiling-and-Running section you were pointed to at the start of this Wiki, quick-start.sh constructs a file called setup_build_environment which does what it says it's going to do. It performs ups setups of dependencies (Boost, etc.) which appfwk specifically, and in general most likely any package you'll write, rely on. If you have any new dependencies which are stored as ups products in the /cvmfs/dune.opensciencegrid.org/dunedaq/DUNE/products/ ups products area, you'll want to add them by looking for the setup_returns variable in setup_build_environment . You'll see there are several packages where for each package, there's a ups setup followed by an appending of the return value of the setup to setup_returns . You should do the same for each dependency you want to add, e.g. setup <dependency_I_am_introducing> <version_of_the_dependency> # And perhaps qualifiers, e.g., -q e19:debug setup_returns=$setup_returns\"$? \" The setup_build_environment script uses setup_returns to check that all the packages were setup without a nonzero return value. You may also want to set up additional environment variables for your new project in setup_build_environment . Installing your project as a local package Use the procedure described below in order to have your package installed. Once your package is installed, it means other packages can access the libraries, public headers, etc., provided by your package via CMake's find_package command, i.e.: # If other users call this, they can use your code find_package(mypackage) For starters, you'll want to call the DAQ module's daq_install function at the bottom of your CMakeLists.txt file. How to do so has already been described earlier in this document. You'll also want to make sure that projects which link in your code know where to find your headers. If you look in toylibrary's CMakeLists.txt file, you'll see the following: target_include_directories(toylibrary PUBLIC $<BUILD_INTERFACE:${CMAKE_CURRENT_SOURCE_DIR}/include> $<INSTALL_INTERFACE:${CMAKE_INSTALL_INCLUDEDIR}> ) ...and what this will do is tell targets in other projects where to find headers when they link in the appfwk library. The code uses CMake generator expressions so that the target will know either to look in a subdirectory of your ./install area or a local build directory depending on whether or not it's using an installed version of appfwk. ${CMAKE_INSTALL_INCLUDEDIR} is set in the GNUInstallDirs module, which is imported as a consequence of importing our DAQ module. The value of the variable defaults to \"include\" and it's unlikely you'd want to change it. A major thing you should be aware of is that when you call CMake's find_package function, it will look for a file with the name mypackageConfig.cmake in a predetermined set of directories. The daq_setup_environment function discussed at the top of this page ensures that one of those directories is your local ./install/mypackage directory. What a standard mypackageConfig.cmake file should look like with modern CMake is documented in many places on the web, but in order to make life as easy as possible there's a templatized version of this file in the daq-buildtools package. Assuming you've got a ./mypackage repo in your development area, you can do the following: cd ./mypackage cp ../daq-buildtools/configs/Config.cmake.in mypackageConfig.cmake.in and then let's look the contents of mypackageConfig.cmake.in : @PACKAGE_INIT@ include(CMakeFindDependencyMacro) # Insert find_dependency() calls for your package's dependencies in # the place of this comment. Make sure they match up with the # find_package calls in your package's CMakeLists.txt file set_and_check(targets_file ${CMAKE_CURRENT_LIST_DIR}/@PROJECT_NAME@Targets.cmake) include(${targets_file}) check_required_components(@PROJECT_NAME@) The only part of this file you need to worry about is the comment. In place of this comment, you'll want to call CMake's find_dependency function (details here ) for each package that mypackage depends on; this ensures that developers who call find_package(mypackage) don't need to have explicit find_package calls on these dependencies. You can see a simple example of this kind of file with ./toylibrary/toylibraryConfig.cmake.in , and a slightly less simple example with ./appfwk/appfwkConfig.cmake.in . Once you've edited this file as described, from the base of your development area you can then run ./build_daq_software.sh --install --pkgname mypackage without receiving an error message informing you that installation isn't an option.","title":"Creating a new package"},{"location":"versions/devel/Creating-a-new-package/#setting-up-a-development-area","text":"To create a new package, you'll want to install a DUNE-DAQ development environment and then create a new CMake project for the package. How to install and build the DUNE-DAQ development environment is described here . By the time you've set up the development environment as described in the linked instructions, you'll have already downloaded, built, and locally installed the CMake project for appfwk. You'll likely also have noticed that ./appfwk is a git repo; you'll want your project to reside in a git repo as well.","title":"Setting up a development area"},{"location":"versions/devel/Creating-a-new-package/#a-packages-subdirectory-structure","text":"To learn a bit more about how to structure your package so that it can be incorporated into the DUNE DAQ software suite, we'll play with a contrived package called \"toylibrary\". It's actually contained within a subdirectory of the daq-buildtools repo; however, in order to be able to build toylibrary we'll want to copy it into the base directory of the development area. Assuming you're already in the base directory, just do cp -rp daq-buildtools/toylibrary . You can now build it using the normal commands: . ./setup_build_environment ./build_daq_software.sh --install --pkgname toylibrary In terms of its actual functionality, it's pretty useless (it contains a class which can wrap an integer, and another class which can print that wrapped integer). However, its functionality is beside the point; toylibrary contains many features which DUNE DAQ packages have in common, in particular DUNE DAQ packages which provide a libraries other developers want to link against. In fact, if you run ls appfwk and ls toylibrary , you'll notice many subdirectories in common. These include: src : contains the source files meant to be built into the package's shared object library/libraries include : contains the headers users of your package should #include unittest : contains the unit tests you write to ensure that your individual classes, functions, etc. behave as expected test : contains any applications you've written for the purpose of integration testing - ensuring that your software components interact as expected If your package contains applications intended not for testing but for the end user, you'd put the code for it in a subdirectory called apps . toylibrary doesn't have this type of application, but the appfwk package does.","title":"A package's subdirectory structure"},{"location":"versions/devel/Creating-a-new-package/#coding-rules","text":"Along with having a standard directory structure, the C++ code itself in toylibrary conforms to the DUNE C++ Style Guide . Here, \"style\" doesn't mean whitespace and formatting, but rather, a set of Modern C++ best practices designed to make your code more robust against bugs, easier to extend, easier to reuse, etc. The DUNE C++ Style Guide is derived from the Google C++ Style Guide, but is greatly simplified and has been modified to be more appropriate to the DUNE DAQ project than Google's projects. Code which is merged into a package's git develop branch should be in conformance with the guide; while it's encouraged for code on a package's unmerged feature branches to also be in conformance, this is less important.","title":"Coding rules"},{"location":"versions/devel/Creating-a-new-package/#your-projects-cmakeliststxt-file","text":"Every DUNE DAQ package should have one and only one CMakeLists.txt file, in the base directory of the package (not to be confused with the base directory of the overall development area). To learn a bit about what that CMakeLists.txt file should look like, let's take a look at ./toylibrary/CMakeLists.txt . Because CMake is widely used and extensively documented online, this documentation will primarily focus on DUNE-specific CMake functions. Before doing anything else, we want to define the minimum version of CMake used (currently 3.12, which supports modern CMake style ) as well as the name and version of the project. Concerning the version: it may not literally be the case that the code you're working with is exactly the same as the version-in-question's release code, because you may be on a feature branch, or there may have been commits to the develop branch since the last release. cmake_minimum_required(VERSION 3.12) project(toylibrary VERSION 1.1.0) Next, we want to make CMake functions written specifically for DUNE DAQ development available. Near the top of the file, you'll see the following: set(CMAKE_MODULE_PATH ${CMAKE_CURRENT_SOURCE_DIR}/../daq-buildtools/cmake ${CMAKE_MODULE_PATH}) include(DAQ) This is currently (Aug-5-2020) how we ensure that the CMakeLists.txt file has access to the standard DUNE DAQ CMake functions. It imports the DAQ CMake module which is located in the daq-buildtools repo that's downloaded when you ran quick-start.sh. Note that by convention all functions/macros within the module begin with daq_ , so as to distinguish them from functions/macros from CMake modules written outside of DUNE DAQ. The next step is to call a macro from the DAQ module which sets up a standard DUNE CMake environment for your CMakeLists.txt file: daq_setup_environment() To get specific, daq_setup_environment() will do the following: * Enforce the use of standard, extension-free C++17 * Ensure libraries are built as shared, rather than static * Ensure all code within the project can find the project's public headers * Allow our linter scripts to work with the code * Tell the CMake find_package function to look in the local ./install subdirectory of the development area for packages (e.g., appfwk) * Have gcc use standard warnings * Support the use of CTest for the unit tests Next you'll see calls to CMake's find_package function, which makes toylibrary's dependencies available. Comments in the file explain why the dependencies are selected. Then, you'll see a call to a function called daq_point_build_to . CMake commands which are specific to the code in a particular subdirectory tree of your project should be clustered in the same location of the the CMakeLists.txt file, and they should be prefaced with the name of the subdirectory wrapped in the daq_point_build_to CMake function. E.g., for the code in ./toylibrary/src we have: daq_point_build_to( src ) add_library( toylibrary src/IntPrinter.cpp ) Here, daq_point_build_to(src) will ensure that the ensuing target(s) -- here, the toylibrary shared object file -- will be placed in a subdirectory of ./build/toylibrary directory (build directory) whose path mirrors the path of the source itself. To get concrete, this command means that after a successful build we'll have an executable, ./build/toylibrary/src/libtoylibrary.so , where that path is given relative to the base of your entire development area. Without this function call it would end up as ./build/toylibrary/libtoylibrary.so . With a small project such as toylibrary the benefit may not be obvious, but it becomes more so when many libraries, applications, unit tests, etc. are built within a project. Another function currently provided by the DAQ CMake module is daq_add_unit_test . Examples of this function's use can be found at the bottom of the toylibrary/CMakeLists.txt file, e.g.: daq_add_unit_test(ValueWrapper_test) If you pass this function a name, e.g., MyComponent_test , it will create a unit test executable off of a source file called <your packagename>/unittest/MyComponent_test.cxx , and handle linking in the Boost unit test dependencies. You can also optionally have it link in other libraries by providing them as arguments; in the above example, this isn't needed because ValueWrapper is a template class which is instantiated within the unit test code itself. You can see this linking, however, in ./appfwk/CMakeLists.txt . At the bottom of CMakeLists.txt, you'll see the following function: daq_install(TARGETS toylibrary) The signature of this function is hopefully self-explanatory; basically when you call it it will install the targets (executables, shared object libraries) you wish to make available to others who want to use your package in a directory called ./install/<pkgname> (here, of course, that would be ./install/toylibrary ). You'll also need to add a special file to your project for this function to work; this is discussed more fully in the \"Installing your project as a local package\" section later in this document.","title":"Your project's CMakeLists.txt file"},{"location":"versions/devel/Creating-a-new-package/#some-general-comments-about-cmake","text":"While a lot can be learned from looking at the CMakeLists.txt file and reading this documentation, realize that CMake is a full-blown programming language, and like any programming language, the more familiar you are with it the easier life will be. It's extensively documented online; in particular, if you want to learn more about specific CMake functions you can go here for a reference guide of CMake commands, or, if you've sourced setup_build_environment (so CMake is set up), you can even learn at the command line via cmake --help-command <name of command> When you write code in CMake good software development principles generally apply. Writing your own macros and functions you can live up to the Don't Repeat Yourself principle ; an example of this can be found in the definition of the MakeDataTypeLibraries macro in ./appfwk/CMakeLists.txt , which helps avoid boilerplate while instantiating appfwk's FanOutDAQModule template class for various types and giving these instantiations well-motivated names. Another thing to be aware of: since you're using a single CMakeLists.txt file, the scope of your decisions can extend to all ensuing code. So, e.g., if you add the line link_libraries( bloated_library_with_extremely_common_symbol_names ) then all ensuing targets from add_executable , add_library , etc., will get that linked in. For this reason, prefer a target-specific call such as target_link_libraries(executable_that_really_needs_this_library bloated_library_with_extremely_common_symbol_names) instead. In modern CMake it's considered a best practice to avoid functions which globally link in libraries or include directories, and to make things target-specific instead.","title":"Some general comments about CMake"},{"location":"versions/devel/Creating-a-new-package/#if-your-package-relies-on-nonstandard-dependencies","text":"As you've discovered from the Compiling-and-Running section you were pointed to at the start of this Wiki, quick-start.sh constructs a file called setup_build_environment which does what it says it's going to do. It performs ups setups of dependencies (Boost, etc.) which appfwk specifically, and in general most likely any package you'll write, rely on. If you have any new dependencies which are stored as ups products in the /cvmfs/dune.opensciencegrid.org/dunedaq/DUNE/products/ ups products area, you'll want to add them by looking for the setup_returns variable in setup_build_environment . You'll see there are several packages where for each package, there's a ups setup followed by an appending of the return value of the setup to setup_returns . You should do the same for each dependency you want to add, e.g. setup <dependency_I_am_introducing> <version_of_the_dependency> # And perhaps qualifiers, e.g., -q e19:debug setup_returns=$setup_returns\"$? \" The setup_build_environment script uses setup_returns to check that all the packages were setup without a nonzero return value. You may also want to set up additional environment variables for your new project in setup_build_environment .","title":"If your package relies on nonstandard dependencies"},{"location":"versions/devel/Creating-a-new-package/#installing-your-project-as-a-local-package","text":"Use the procedure described below in order to have your package installed. Once your package is installed, it means other packages can access the libraries, public headers, etc., provided by your package via CMake's find_package command, i.e.: # If other users call this, they can use your code find_package(mypackage) For starters, you'll want to call the DAQ module's daq_install function at the bottom of your CMakeLists.txt file. How to do so has already been described earlier in this document. You'll also want to make sure that projects which link in your code know where to find your headers. If you look in toylibrary's CMakeLists.txt file, you'll see the following: target_include_directories(toylibrary PUBLIC $<BUILD_INTERFACE:${CMAKE_CURRENT_SOURCE_DIR}/include> $<INSTALL_INTERFACE:${CMAKE_INSTALL_INCLUDEDIR}> ) ...and what this will do is tell targets in other projects where to find headers when they link in the appfwk library. The code uses CMake generator expressions so that the target will know either to look in a subdirectory of your ./install area or a local build directory depending on whether or not it's using an installed version of appfwk. ${CMAKE_INSTALL_INCLUDEDIR} is set in the GNUInstallDirs module, which is imported as a consequence of importing our DAQ module. The value of the variable defaults to \"include\" and it's unlikely you'd want to change it. A major thing you should be aware of is that when you call CMake's find_package function, it will look for a file with the name mypackageConfig.cmake in a predetermined set of directories. The daq_setup_environment function discussed at the top of this page ensures that one of those directories is your local ./install/mypackage directory. What a standard mypackageConfig.cmake file should look like with modern CMake is documented in many places on the web, but in order to make life as easy as possible there's a templatized version of this file in the daq-buildtools package. Assuming you've got a ./mypackage repo in your development area, you can do the following: cd ./mypackage cp ../daq-buildtools/configs/Config.cmake.in mypackageConfig.cmake.in and then let's look the contents of mypackageConfig.cmake.in : @PACKAGE_INIT@ include(CMakeFindDependencyMacro) # Insert find_dependency() calls for your package's dependencies in # the place of this comment. Make sure they match up with the # find_package calls in your package's CMakeLists.txt file set_and_check(targets_file ${CMAKE_CURRENT_LIST_DIR}/@PROJECT_NAME@Targets.cmake) include(${targets_file}) check_required_components(@PROJECT_NAME@) The only part of this file you need to worry about is the comment. In place of this comment, you'll want to call CMake's find_dependency function (details here ) for each package that mypackage depends on; this ensures that developers who call find_package(mypackage) don't need to have explicit find_package calls on these dependencies. You can see a simple example of this kind of file with ./toylibrary/toylibraryConfig.cmake.in , and a slightly less simple example with ./appfwk/appfwkConfig.cmake.in . Once you've edited this file as described, from the base of your development area you can then run ./build_daq_software.sh --install --pkgname mypackage without receiving an error message informing you that installation isn't an option.","title":"Installing your project as a local package"},{"location":"versions/devel/Step-by-step-instructions-for-creating-your-own-DAQModule-package/","text":"Introduction This page is intended to provide a step-by-step guide to setting up a work area in which you create software modules that run within the DUNE DAQ software application framework ( appfwk ). These modules are often called DAQModules since that is the C++ base class that they inherit from. If you are impatient to get started, you can jump to the [[Step-by-step Instructions|Step-by-step-instructions-for-creating-your-own-DAQModule-package#step-by-step Installation instructions]] now. New users should probably continue reading this introduction, though, to learn a little bit of background information. This page draws from, and references, several other Wiki pages in the appfwk repository. Links to these pages are listed in the text in the relevant places, and they are a good source of additional information for interested readers. Here are some of these references collected for you in one place on this page: * [[Compiling and running the App Fwk|Compiling-and-running]] * [[Creating a new package|Instructions-for-creating-a-new-package]] and last, but certainly not least * The DUNE DAQ C++ Style Guide This page assumes a basic understanding of how modules are combined to form processes within appfwk v1. A brief recap can be found in the following snippet. A reference for this is the sneak peak talk given by Eric Flumerfelt on 15-Jun-2020. As a refresher: * a *DAQProcess* contains one or more *DAQModules* * when multiple *DAQModules* are present within a *DAQProcess*, they communicate with each other via *Queues* (the diagrams that are included [[later on this page|Step-by-step-instructions-for-creating-your-own-DAQModule-package#information-about-the-daqmodules-in-the-listrev-example]] might help explain this) * there are two classes that wrap *Queues* to provide the ability to push data onto the queue (*DAQSink*) or pull data from a queue (*DAQSource*). An individual *DAQModule* will only access one side of each *Queue*. If the module pushes data onto the queue, it will use an instance of the *DAQSink* class (which wraps the desired *Queue*), and if the module pops data from the queue, it will use an instance of the *DAQSource* class. * the *DAQModules* that run within a given process (and the *Queues* between them) are specified in a JSON *process configuration* file. An example of one such file is given below. The creation of the *DAQModules* and *Queues* is handled by the *appfwk*. * at this point in time, we expect that most users will be developing *DAQModules* and simply using existing *Queues* from the *appfwk*. * at the moment, there aren't any centrally-provided libraries, tools, or recommendations for inter-process communication. We expect to address this topic soon, but for now, developers can either focus on single-process examples, or use other software for inter-process communication. The instructions on this page focus on single-process examples. * please remember that there have been a number of compromises or simplifications in the functionality that is provided by *appfwk* v1. We have made a good faith attempt to provide a good start on the internal interfaces and layout of *DAQProcesses*, but things will definitely change over time, and we will be gathering feedback from everyone on ways that things might be improved. (Of course, we already have a fairly good list based on the experience of creating v1.) Step-by-step Installation instructions Instructions on setting up a work area in which you can create your new package can be found [[here|Compiling-and-running]]. After you've gone through those instructions, for our purposes the first build command we'll want to run will one which installs appfwk in a local ./install subdirectory. * ./build_daq_software.sh --install You can then link your own code against your local installation of appfwk. * If you would like to try running the example application(s) from the core appfwk repository, you can do that now. There are instructions [[later on this page|Step-by-step-instructions-for-creating-your-own-DAQModule-package#running-the-fanout-example-in-the-appfwk-repo]] for doing that. At this point, you could either check out the example DAQModule package, build it, and run the example application (recommended), or you could jump to [[creating your own package|Step-by-step-instructions-for-creating-your-own-DAQModule-package#Create-your-own-software-package]]. These instructions will walk you through doing both, but of course you can skip to the latter by scrolling to the dedicated section. Install the example package Here are the steps for adding the appfwk Example package to your work area, building it, and running the example application... (For information on the DAQModules that are contained in the example application and how they interact, please see [[this section|Step-by-step-instructions-for-creating-your-own-DAQModule-package#Information-about-the-DAQModules-in-the-listrev-example]] later on this page.) * cd into your work area directory ( WORK_DIR ), if you aren't there already * run the following command to clone the example package: * git clone https://github.com/DUNE-DAQ/listrev.git * [if not already done in the current shell] set up the build environment for your work area * source ./setup_build_environment * Build the listrev in your work area by running * ./build_daq_software.sh --pkgname listrev In order to run the example package follow the instruction in the [[dedicated section|Step-by-step-instructions-for-creating-your-own-DAQModule-package#Running the example package]]. Create your own software package Here are the commands to create your own software package that depends on appfwk: * create a new directory underneath WORK_DIR (we'll call this YOUR_PKG_DIR ) * copy the CMakeLists.txt file from the example package into YOUR_PKG_DIR (you can fetch it from here ) * edit this CMakeLists.txt file to change all of the instances of the string \"listrev\" to your package name * (we'll do a couple more edits in a bit, but you can save and close this file now * create a src directory underneath YOUR_PKG_DIR * copy one of the DAQModules from the example package into the src directory * as an example, let's copy the RandomDataListGenerator DAQModule from the example package, along with the CommonIssues header file. You can fetch these files from here , here , and here . * change all instances of \"listrev\" and \"LISTREV\" in these files to your package name * edit YOUR_PKG_DIR /CMakeLists.txt to remove the ListReverser and ReversedListValidator add_library and target_link_libraries lines * you should also comment out the daq_point_build_to( test ) , file(COPY test/list_reversal_app.json DESTINATION test) and daq_install(... lines * cd to WORK_DIR * [if not already done in the current shell] set up the build environment for your work area * source ./setup_build_environment * rebuild your package in your work area by running * ./build_daq_software.sh --pkgname <your package name> * at this point, you'll have a decent start on your own DAQModule package. Of course, you'll need to rename and modify the RandomDataListGenerator DAQModule to do whatever you want your first DAQModule to do. And, when you get the point of running your DAQProcess that uses your DAQModule (s), you'll need to create a JSON process configuration file to use for that, but hopefully the listrev example package will help. How to write a DAQModule From the C++ point of view, DAQ modules are implementation of a [[DAQ Module interface|DAQModules]]. That means that apart from the constructor (that receives a name) only one method has to be implemented: init() . The function has to handle all those configuration items that are not supposed to be reconfigurable during the run. Optionally it can configure all those variables that can be overridden during run time. The most important thing that init has to do is to call register_command in order to associate functions to each command that the module is expected to support. Each function has to be a set of operations that changes the internal status of the module according to the command that it is implementing. The prototype of each function is in the form void do_something( const vector<string> & args ) ; The function can be virtual and the system is able to pick the most downstream specification. Assuming that the name of the module you are developing is MyNewDAQModule a call to associate the command \"start\" to a function called do_start will be register_command(\"start\", & MyNewDAQModule::do_start); It is expected that the operations of the DAQ Module are carried on in other threads under the control of the DAQModule itself. How to make your DAQModule a plugin of the appfmw DAQModules need to be created with a factory pattern simply via their name. In the application framework this is done using CETlib . In order to do that, each implementation ( cpp file) needs to have a call to the macro DEFINE_DUNE_DAQ_MODULE(<name_including_namespace>) for example DEFINE_DUNE_DAQ_MODULE(dunedaq::appfwk::DummyModule) In order to generate a shared object library that is linkable in runtime from the application framework, the name of the library has to be the in the form <package>_<module_name_no_namespace>_duneDAQModule . This can be achieved simply adding a line in the CMakeLists.txt file of your project in the form add_library(<package>_<module_name_no_namespace>_duneDAQModule path/to/my/file.cpp) . For example: add_library(appfwk_DummyModule_duneDAQModule test/DummyModule.cpp) Running examples Running the example package Assuming you installed your software as described in the previous sections, these are the instructions to run the code: * set up the runtime environment setup script by running this command: * source ./setup_runtime_environment * please note that this needs to be done from WORK_DIR * run the example application using the following command: * daq_application -c QueryResponseCommandFacility -j build/listrev/test/list_reversal_app.json * once the program is running, and you see the \"Enter a command\" prompt, you can type in commands like the following: * configure * This command pretends to set the values of configurable parameters like the number of integers in each randomly generated list * start * This command is passed to the three DAQModules in a well-specified order (specified in the JSON process configuration file). First, the validator is started, then the reverser , then the generator . Once the generator is started, it begins creating lists of integers and passing them to the other two DAQModules . * On the console, you will see ERS LOG messages from each of the DAQModules saying that they started (so you can confirm that the start order is correct), and then you will see ERS DEBUG messages that tell you what each of the DAQModules is doing as they process the lists of integers. * stop * This command stops the three DAQModules , in the reverse order that they were started (like the start order, the stop order is specified in the JSON process config file). * You will need to type this command into the console a little blindly, since the ERS DEBUG messages will be printing to the console as the program runs. * After each of the three DAQModules has finished, they print an ERS INFO message to the console with a summary of what they accomplished. * unconfigure * This command pretends to tear-down whatever configuration was established in the configure step. * quit * This command exits the program. Information about the DAQModules in the listrev example The idea behind the listrev example is to have one DAQModule that generates a list of random integers, another DAQModule that reverses a copy of the list, and a third DAQModule that compares copies of the original and reversed lists to validate that they are equivalent, modulo the reversal. This is shown in the following diagrams. The first one provides a little description of what each of the DAQModules is doing, and the second one shows the class names of the DAQModules and the configured names of the DAQModules and the Queues , as they are identified in the JSON process configuration file. A copy of the listrev JSON process configuration file is shown below (the official copy of this file in the repo is here ). As you can see, the Queues are specified first, then the DAQModules , and the DAQModules include the configuration of which Queues they make use of. * The capacity and kind parameters within the queue declarations are required. The names of the queues (e.g. primaryDataQueue ) are your choice. * The user_module_type parameter in the module declarations is required. The names of the parameters that specify the queues to the modules are up to you. As you can see in this example, different parameter names are used to specify the queues to the three different modules. (Of course, what the DAQModule code does with the queue names is standardized. An example of that is shown in this snippet of listrev code .) { \"queues\": { \"primaryDataQueue\": { \"capacity\": 10, \"kind\": \"FollySPSCQueue\" }, \"reversedDataQueue\": { \"capacity\": 10, \"kind\": \"FollySPSCQueue\" }, \"dataCopyQueue\": { \"capacity\": 10, \"kind\": \"FollySPSCQueue\" } }, \"modules\": { \"generator\": { \"user_module_type\": \"RandomDataListGenerator\", \"outputs\": [ \"primaryDataQueue\", \"dataCopyQueue\" ] }, \"reverser\": { \"user_module_type\": \"ListReverser\", \"input\": \"primaryDataQueue\", \"output\": \"reversedDataQueue\" }, \"validator\": { \"user_module_type\": \"ReversedListValidator\", \"reversed_data_input\": \"reversedDataQueue\", \"original_data_input\": \"dataCopyQueue\" } }, \"commands\": { \"start\": [ \"validator\", \"reverser\", \"generator\" ], \"stop\": [ \"generator\", \"reverser\", \"validator\" ] } } As mentioned earlier on this page, when you run the listrev example, you will see ERS messages printed out in the console. ERS is the Error Reporting Service from the ATLAS experiment. Within the DUNE DAQ, we have our own fork of that package, and you will see a clone of the DUNE DAQ ERS repo when you look at the directories underneath WORK_DIR . For further information on ERS, you can see this journal article (PDF) . In addition to ERS messages, there are TRACE messages in the listrev example code. The current model for using ERS and TRACE is described in a section of the Style Guide , and the example code follows those guidelines. For example, the example DAQModule s usee ERS Issues for warnings, errors, and fatal errors, and TRACE messages for messages that developers would use for debugging or verifying the behavior of the code. The periodic progress reports are implemented as ERS DEBUG messages, although admittedly, they could just as easily been implemented as TRACE messages. The choice there was simply a practical one - that ERS messages are displayed on the console by default, whereas TRACE messages typically go to a memory area by default so that they use very little system resources. (TRACE messages can trivially be directed to the console, but that would have been just one more step in the instructions above.) Users who are interested in seeing the TRACE messages from the listrev example code, or their own DAQModules when the time comes, can use the following steps: * before running the program, set the TRACE_FILE environmental variable to point to a file underneath your WORK_DIR * export TRACE_FILE=<WORK_DIR>/log/${USER}_dunedaq.trace * run the program * look at the TRACE levels that are enabled for each TRACE_NAME (TRACE_NAMEs are used to help identify which source file the messages were sent from) * tlvls * enable the TRACE levels that you would like to see appear in the TRACE memory buffer with commands like the following: * tonM -n RandomDataListGenerator 10 * tonM -n RandomDataListGenerator 15 * view the messages in the TRACE memory buffer. I appreciate seeing the timestamps in human-readable form, so I typically pipe the output of tshow to tdelta as shown here (both are provided by the TRACE package) * tshow | tdelta -ct 1 | more * Note that the messages from the TRACE buffer are displayed in reverse time order (most recent message first) This short introduction to TRACE only describes a small fraction of its capabilities, and interested users are encouraged to read the Quick Start guide, the User's Guide, and other documentation provided here . Running the Fanout example in the appfwk repo In a fresh shell, here are the steps that you would use to run the Fanout example in the appfwk repo after you have run quick-start.sh and built the software using build_daq_software.sh : * cd to your WORK_DIR * run the following commands to set up the build and runtime environments: * source ./setup_build_environment * source ./setup_runtime_environment * run the following command to start the example: * build/appfwk/apps/daq_application -c QueryResponseCommandFacility -j appfwk/test/producer_consumer_dynamic_test.json * enter commands like the following: * configure * start * stop * quit","title":"Introduction"},{"location":"versions/devel/Step-by-step-instructions-for-creating-your-own-DAQModule-package/#introduction","text":"This page is intended to provide a step-by-step guide to setting up a work area in which you create software modules that run within the DUNE DAQ software application framework ( appfwk ). These modules are often called DAQModules since that is the C++ base class that they inherit from. If you are impatient to get started, you can jump to the [[Step-by-step Instructions|Step-by-step-instructions-for-creating-your-own-DAQModule-package#step-by-step Installation instructions]] now. New users should probably continue reading this introduction, though, to learn a little bit of background information. This page draws from, and references, several other Wiki pages in the appfwk repository. Links to these pages are listed in the text in the relevant places, and they are a good source of additional information for interested readers. Here are some of these references collected for you in one place on this page: * [[Compiling and running the App Fwk|Compiling-and-running]] * [[Creating a new package|Instructions-for-creating-a-new-package]] and last, but certainly not least * The DUNE DAQ C++ Style Guide This page assumes a basic understanding of how modules are combined to form processes within appfwk v1. A brief recap can be found in the following snippet. A reference for this is the sneak peak talk given by Eric Flumerfelt on 15-Jun-2020. As a refresher: * a *DAQProcess* contains one or more *DAQModules* * when multiple *DAQModules* are present within a *DAQProcess*, they communicate with each other via *Queues* (the diagrams that are included [[later on this page|Step-by-step-instructions-for-creating-your-own-DAQModule-package#information-about-the-daqmodules-in-the-listrev-example]] might help explain this) * there are two classes that wrap *Queues* to provide the ability to push data onto the queue (*DAQSink*) or pull data from a queue (*DAQSource*). An individual *DAQModule* will only access one side of each *Queue*. If the module pushes data onto the queue, it will use an instance of the *DAQSink* class (which wraps the desired *Queue*), and if the module pops data from the queue, it will use an instance of the *DAQSource* class. * the *DAQModules* that run within a given process (and the *Queues* between them) are specified in a JSON *process configuration* file. An example of one such file is given below. The creation of the *DAQModules* and *Queues* is handled by the *appfwk*. * at this point in time, we expect that most users will be developing *DAQModules* and simply using existing *Queues* from the *appfwk*. * at the moment, there aren't any centrally-provided libraries, tools, or recommendations for inter-process communication. We expect to address this topic soon, but for now, developers can either focus on single-process examples, or use other software for inter-process communication. The instructions on this page focus on single-process examples. * please remember that there have been a number of compromises or simplifications in the functionality that is provided by *appfwk* v1. We have made a good faith attempt to provide a good start on the internal interfaces and layout of *DAQProcesses*, but things will definitely change over time, and we will be gathering feedback from everyone on ways that things might be improved. (Of course, we already have a fairly good list based on the experience of creating v1.)","title":"Introduction"},{"location":"versions/devel/Step-by-step-instructions-for-creating-your-own-DAQModule-package/#step-by-step-installation-instructions","text":"Instructions on setting up a work area in which you can create your new package can be found [[here|Compiling-and-running]]. After you've gone through those instructions, for our purposes the first build command we'll want to run will one which installs appfwk in a local ./install subdirectory. * ./build_daq_software.sh --install You can then link your own code against your local installation of appfwk. * If you would like to try running the example application(s) from the core appfwk repository, you can do that now. There are instructions [[later on this page|Step-by-step-instructions-for-creating-your-own-DAQModule-package#running-the-fanout-example-in-the-appfwk-repo]] for doing that. At this point, you could either check out the example DAQModule package, build it, and run the example application (recommended), or you could jump to [[creating your own package|Step-by-step-instructions-for-creating-your-own-DAQModule-package#Create-your-own-software-package]]. These instructions will walk you through doing both, but of course you can skip to the latter by scrolling to the dedicated section.","title":"Step-by-step Installation instructions"},{"location":"versions/devel/Step-by-step-instructions-for-creating-your-own-DAQModule-package/#install-the-example-package","text":"Here are the steps for adding the appfwk Example package to your work area, building it, and running the example application... (For information on the DAQModules that are contained in the example application and how they interact, please see [[this section|Step-by-step-instructions-for-creating-your-own-DAQModule-package#Information-about-the-DAQModules-in-the-listrev-example]] later on this page.) * cd into your work area directory ( WORK_DIR ), if you aren't there already * run the following command to clone the example package: * git clone https://github.com/DUNE-DAQ/listrev.git * [if not already done in the current shell] set up the build environment for your work area * source ./setup_build_environment * Build the listrev in your work area by running * ./build_daq_software.sh --pkgname listrev In order to run the example package follow the instruction in the [[dedicated section|Step-by-step-instructions-for-creating-your-own-DAQModule-package#Running the example package]].","title":"Install the example package"},{"location":"versions/devel/Step-by-step-instructions-for-creating-your-own-DAQModule-package/#create-your-own-software-package","text":"Here are the commands to create your own software package that depends on appfwk: * create a new directory underneath WORK_DIR (we'll call this YOUR_PKG_DIR ) * copy the CMakeLists.txt file from the example package into YOUR_PKG_DIR (you can fetch it from here ) * edit this CMakeLists.txt file to change all of the instances of the string \"listrev\" to your package name * (we'll do a couple more edits in a bit, but you can save and close this file now * create a src directory underneath YOUR_PKG_DIR * copy one of the DAQModules from the example package into the src directory * as an example, let's copy the RandomDataListGenerator DAQModule from the example package, along with the CommonIssues header file. You can fetch these files from here , here , and here . * change all instances of \"listrev\" and \"LISTREV\" in these files to your package name * edit YOUR_PKG_DIR /CMakeLists.txt to remove the ListReverser and ReversedListValidator add_library and target_link_libraries lines * you should also comment out the daq_point_build_to( test ) , file(COPY test/list_reversal_app.json DESTINATION test) and daq_install(... lines * cd to WORK_DIR * [if not already done in the current shell] set up the build environment for your work area * source ./setup_build_environment * rebuild your package in your work area by running * ./build_daq_software.sh --pkgname <your package name> * at this point, you'll have a decent start on your own DAQModule package. Of course, you'll need to rename and modify the RandomDataListGenerator DAQModule to do whatever you want your first DAQModule to do. And, when you get the point of running your DAQProcess that uses your DAQModule (s), you'll need to create a JSON process configuration file to use for that, but hopefully the listrev example package will help.","title":"Create your own software package"},{"location":"versions/devel/Step-by-step-instructions-for-creating-your-own-DAQModule-package/#how-to-write-a-daqmodule","text":"From the C++ point of view, DAQ modules are implementation of a [[DAQ Module interface|DAQModules]]. That means that apart from the constructor (that receives a name) only one method has to be implemented: init() . The function has to handle all those configuration items that are not supposed to be reconfigurable during the run. Optionally it can configure all those variables that can be overridden during run time. The most important thing that init has to do is to call register_command in order to associate functions to each command that the module is expected to support. Each function has to be a set of operations that changes the internal status of the module according to the command that it is implementing. The prototype of each function is in the form void do_something( const vector<string> & args ) ; The function can be virtual and the system is able to pick the most downstream specification. Assuming that the name of the module you are developing is MyNewDAQModule a call to associate the command \"start\" to a function called do_start will be register_command(\"start\", & MyNewDAQModule::do_start); It is expected that the operations of the DAQ Module are carried on in other threads under the control of the DAQModule itself.","title":"How to write a DAQModule"},{"location":"versions/devel/Step-by-step-instructions-for-creating-your-own-DAQModule-package/#how-to-make-your-daqmodule-a-plugin-of-the-appfmw","text":"DAQModules need to be created with a factory pattern simply via their name. In the application framework this is done using CETlib . In order to do that, each implementation ( cpp file) needs to have a call to the macro DEFINE_DUNE_DAQ_MODULE(<name_including_namespace>) for example DEFINE_DUNE_DAQ_MODULE(dunedaq::appfwk::DummyModule) In order to generate a shared object library that is linkable in runtime from the application framework, the name of the library has to be the in the form <package>_<module_name_no_namespace>_duneDAQModule . This can be achieved simply adding a line in the CMakeLists.txt file of your project in the form add_library(<package>_<module_name_no_namespace>_duneDAQModule path/to/my/file.cpp) . For example: add_library(appfwk_DummyModule_duneDAQModule test/DummyModule.cpp)","title":"How to make your DAQModule a plugin of the appfmw"},{"location":"versions/devel/Step-by-step-instructions-for-creating-your-own-DAQModule-package/#running-examples","text":"","title":"Running examples"},{"location":"versions/devel/Step-by-step-instructions-for-creating-your-own-DAQModule-package/#running-the-example-package","text":"Assuming you installed your software as described in the previous sections, these are the instructions to run the code: * set up the runtime environment setup script by running this command: * source ./setup_runtime_environment * please note that this needs to be done from WORK_DIR * run the example application using the following command: * daq_application -c QueryResponseCommandFacility -j build/listrev/test/list_reversal_app.json * once the program is running, and you see the \"Enter a command\" prompt, you can type in commands like the following: * configure * This command pretends to set the values of configurable parameters like the number of integers in each randomly generated list * start * This command is passed to the three DAQModules in a well-specified order (specified in the JSON process configuration file). First, the validator is started, then the reverser , then the generator . Once the generator is started, it begins creating lists of integers and passing them to the other two DAQModules . * On the console, you will see ERS LOG messages from each of the DAQModules saying that they started (so you can confirm that the start order is correct), and then you will see ERS DEBUG messages that tell you what each of the DAQModules is doing as they process the lists of integers. * stop * This command stops the three DAQModules , in the reverse order that they were started (like the start order, the stop order is specified in the JSON process config file). * You will need to type this command into the console a little blindly, since the ERS DEBUG messages will be printing to the console as the program runs. * After each of the three DAQModules has finished, they print an ERS INFO message to the console with a summary of what they accomplished. * unconfigure * This command pretends to tear-down whatever configuration was established in the configure step. * quit * This command exits the program.","title":"Running the example package"},{"location":"versions/devel/Step-by-step-instructions-for-creating-your-own-DAQModule-package/#information-about-the-daqmodules-in-the-listrev-example","text":"The idea behind the listrev example is to have one DAQModule that generates a list of random integers, another DAQModule that reverses a copy of the list, and a third DAQModule that compares copies of the original and reversed lists to validate that they are equivalent, modulo the reversal. This is shown in the following diagrams. The first one provides a little description of what each of the DAQModules is doing, and the second one shows the class names of the DAQModules and the configured names of the DAQModules and the Queues , as they are identified in the JSON process configuration file. A copy of the listrev JSON process configuration file is shown below (the official copy of this file in the repo is here ). As you can see, the Queues are specified first, then the DAQModules , and the DAQModules include the configuration of which Queues they make use of. * The capacity and kind parameters within the queue declarations are required. The names of the queues (e.g. primaryDataQueue ) are your choice. * The user_module_type parameter in the module declarations is required. The names of the parameters that specify the queues to the modules are up to you. As you can see in this example, different parameter names are used to specify the queues to the three different modules. (Of course, what the DAQModule code does with the queue names is standardized. An example of that is shown in this snippet of listrev code .) { \"queues\": { \"primaryDataQueue\": { \"capacity\": 10, \"kind\": \"FollySPSCQueue\" }, \"reversedDataQueue\": { \"capacity\": 10, \"kind\": \"FollySPSCQueue\" }, \"dataCopyQueue\": { \"capacity\": 10, \"kind\": \"FollySPSCQueue\" } }, \"modules\": { \"generator\": { \"user_module_type\": \"RandomDataListGenerator\", \"outputs\": [ \"primaryDataQueue\", \"dataCopyQueue\" ] }, \"reverser\": { \"user_module_type\": \"ListReverser\", \"input\": \"primaryDataQueue\", \"output\": \"reversedDataQueue\" }, \"validator\": { \"user_module_type\": \"ReversedListValidator\", \"reversed_data_input\": \"reversedDataQueue\", \"original_data_input\": \"dataCopyQueue\" } }, \"commands\": { \"start\": [ \"validator\", \"reverser\", \"generator\" ], \"stop\": [ \"generator\", \"reverser\", \"validator\" ] } } As mentioned earlier on this page, when you run the listrev example, you will see ERS messages printed out in the console. ERS is the Error Reporting Service from the ATLAS experiment. Within the DUNE DAQ, we have our own fork of that package, and you will see a clone of the DUNE DAQ ERS repo when you look at the directories underneath WORK_DIR . For further information on ERS, you can see this journal article (PDF) . In addition to ERS messages, there are TRACE messages in the listrev example code. The current model for using ERS and TRACE is described in a section of the Style Guide , and the example code follows those guidelines. For example, the example DAQModule s usee ERS Issues for warnings, errors, and fatal errors, and TRACE messages for messages that developers would use for debugging or verifying the behavior of the code. The periodic progress reports are implemented as ERS DEBUG messages, although admittedly, they could just as easily been implemented as TRACE messages. The choice there was simply a practical one - that ERS messages are displayed on the console by default, whereas TRACE messages typically go to a memory area by default so that they use very little system resources. (TRACE messages can trivially be directed to the console, but that would have been just one more step in the instructions above.) Users who are interested in seeing the TRACE messages from the listrev example code, or their own DAQModules when the time comes, can use the following steps: * before running the program, set the TRACE_FILE environmental variable to point to a file underneath your WORK_DIR * export TRACE_FILE=<WORK_DIR>/log/${USER}_dunedaq.trace * run the program * look at the TRACE levels that are enabled for each TRACE_NAME (TRACE_NAMEs are used to help identify which source file the messages were sent from) * tlvls * enable the TRACE levels that you would like to see appear in the TRACE memory buffer with commands like the following: * tonM -n RandomDataListGenerator 10 * tonM -n RandomDataListGenerator 15 * view the messages in the TRACE memory buffer. I appreciate seeing the timestamps in human-readable form, so I typically pipe the output of tshow to tdelta as shown here (both are provided by the TRACE package) * tshow | tdelta -ct 1 | more * Note that the messages from the TRACE buffer are displayed in reverse time order (most recent message first) This short introduction to TRACE only describes a small fraction of its capabilities, and interested users are encouraged to read the Quick Start guide, the User's Guide, and other documentation provided here .","title":"Information about the DAQModules in the listrev example"},{"location":"versions/devel/Step-by-step-instructions-for-creating-your-own-DAQModule-package/#running-the-fanout-example-in-the-appfwk-repo","text":"In a fresh shell, here are the steps that you would use to run the Fanout example in the appfwk repo after you have run quick-start.sh and built the software using build_daq_software.sh : * cd to your WORK_DIR * run the following commands to set up the build and runtime environments: * source ./setup_build_environment * source ./setup_runtime_environment * run the following command to start the example: * build/appfwk/apps/daq_application -c QueryResponseCommandFacility -j appfwk/test/producer_consumer_dynamic_test.json * enter commands like the following: * configure * start * stop * quit","title":"Running the Fanout example in the appfwk repo"},{"location":"versions/devel/Testing/","text":"Broadly speaking, there are two types of test program you can run: unit tests, and integration tests. Unit tests Unit tests are standalone programs with names of the form <code unit under test>_test and are implemented in the ./unittest subdirectory of a package. They're written using the Boost.Test library. For complete documentation on the Boost.Test library, you can look at the Boost documentation ; for substantially quicker but still useful intros, take a look here and here . While you can run a unit test program without arguments, some arguments are very useful - in particular, those that control which test cases are run, and those that control the level of logging output. To maximize the output to screen, add the argument -l all , and to minimize it, add -l nothing . Other intermediate levels are covered in the Boost documentation. To skip a test, you can add an argument of the form --run_test=\\!test_you_dont_want ; note the exclamation point needs to be escaped with a backslash since the shell interprets it as a special character otherwise. Integration tests Integration tests look at how various units of code interact; unlike unit tests, there's no common approach to running them. As such, a list of integration tests and how to run them can be found in the list below: [[Queue testing|Queue-testing]] dummy_test_app: A simple application that statically instantiates a DummyModule DAQModule, which simply takes the command \"stuff\" to echo a string producer_consumer_dynamic_test.json: A sample configuration file for daq_application which runs a version of the [[Simple readout application|Simple-readout-application]]","title":"Testing"},{"location":"versions/devel/Testing/#unit-tests","text":"Unit tests are standalone programs with names of the form <code unit under test>_test and are implemented in the ./unittest subdirectory of a package. They're written using the Boost.Test library. For complete documentation on the Boost.Test library, you can look at the Boost documentation ; for substantially quicker but still useful intros, take a look here and here . While you can run a unit test program without arguments, some arguments are very useful - in particular, those that control which test cases are run, and those that control the level of logging output. To maximize the output to screen, add the argument -l all , and to minimize it, add -l nothing . Other intermediate levels are covered in the Boost documentation. To skip a test, you can add an argument of the form --run_test=\\!test_you_dont_want ; note the exclamation point needs to be escaped with a backslash since the shell interprets it as a special character otherwise.","title":"Unit tests"},{"location":"versions/devel/Testing/#integration-tests","text":"Integration tests look at how various units of code interact; unlike unit tests, there's no common approach to running them. As such, a list of integration tests and how to run them can be found in the list below: [[Queue testing|Queue-testing]] dummy_test_app: A simple application that statically instantiates a DummyModule DAQModule, which simply takes the command \"stuff\" to echo a string producer_consumer_dynamic_test.json: A sample configuration file for daq_application which runs a version of the [[Simple readout application|Simple-readout-application]]","title":"Integration tests"},{"location":"versions/v1.0.0/Compiling-and-running/","text":"n.b. Under v1.0.0 the build system was considerably different than under later releases (including v1.1.0). Keep in mind that the appfwk package's code is essentially the same between v1.0.0 and v1.1.0, and it's therefore recommended you go with the v1.1.0 instructions. However, if you _do wish to work with the original v1.0.0 release, follow [[Step-by-step instructions for creating your own DAQModule package under v1|Step-by-step-instructions-for-creating-your-own-DAQModule-package-under-v1]]_","title":"Compiling and running"},{"location":"versions/v1.0.0/Creating-a-new-package/","text":"Setting up a development area To create a new package, you'll want to install a DUNE-DAQ development environment and then create a new CMake project for the package. How to install and build the DUNE-DAQ development environment is described [[here|Compiling-and-running]]. After installation of the development environment, you'll see that there are already two CMake projects installed, appfwk and ers (daq-buildtools, which provides CMake modules for DUNE DAQ developers, is not itself a CMake project). You'll also notice that ers and appfwk are git repos; you'll want your project to reside in a git repo as well. The superproject CMakeLists.txt Every project which is installed in your development area is actually a subproject of the dune-day CMake superproject. The superproject's CMakeLists.txt file is constructed when you run quick-start.sh , and resides in the directory out of which you ran it. The CMake code in that file globally influences the CMake code of each project which is a subdirectory; this influence includes: Ensuring that all code is compiled under extension-free, standard C++17 Ensuring that a full set of standard compiler warnings are used Ensuring that the DUNE C++ code linters can work with your C++ code Ensuring that the CMake functions defined in daq-buildtools/CMake/DAQ.cmake are available to your CMake code Ensuring that you won't need to write any CMake code in order for your C++ code to find headers associated with Boost, or the appfwk or ers projects Providing two CMake variables, DAQ_LIBRARIES_UNIVERSAL and DAQ_LIBRARIES_UNIVERSAL_EXE , which define dependencies that any typical DUNE DAQ library ( DAQ_LIBRARIES_UNIVERSAL ) or executable ( DAQ_LIBRARIES_UNIVERSAL_EXE ) should link in You'll also notice that the ers and appfwk projects are added in the superproject CMakeLists.txt via the passing of their namesake subdirectories to CMake's add_subdirectory command; once you have your project subdirectory, you'll want to add it to the superproject CMakeLists.txt file as well so it knows your work should be part of the overall build. Your project's CMakeLists.txt file If you're in the directory in which you ran quick-start.sh , you can look at ers/CMakeLists.txt and appfwk/CMakeLists.txt for examples of project CMakeLists.txt files. While a lot can be learned from looking at these examples and reading this documentation, realize that CMake is a full-blown programming language, and like any programming language, the more familiar you are with it the easier life will be. It's extensively documented online; in particular, if you want to learn more about specific CMake functions you can go here for a reference guide of CMake commands, or, if you've sourced setup_build_environment (so CMake is set up), you can even learn at the command line via cmake --help-command <name of command> However, on top of the CMake language itself, there are CMake functions which have been written specifically for DUNE DAQ development, and there's a structure your CMake code should follow; looking at appfwk/CMakeLists.txt is a good way to learn about both of these. Guidelines to follow are: There there should be only one CMakeLists.txt file in your project, and it should be in the base directory of your project. It's recommended that you link the libraries defined by DAQ_LIBRARIES_UNIVERSAL into the libraries you create, and DAQ_LIBRARIES_UNIVERSAL_EXE into the executables you create. An example of the latter is shown in the 'target_link_libraries' line below. You may also want to define a DAQ_LIBRARIES_PACKAGE CMake variable, which define additional libraries your package's executables and libraries should generally depend on CMake commands which are specific to the code in a subdirectory tree should be clustered in the same location, and they should be prefaced with the name of the subdirectory wrapped in the point_build_to CMake function. E.g., for the code in appfwk/apps we have: point_build_to( apps ) add_executable(daq_application apps/daq_application.cxx) target_link_libraries(daq_application ${DAQ_LIBRARIES_UNIVERSAL_EXE} ${DAQ_LIBRARIES_PACKAGE}) Here, point_build_to(apps) will ensure that the ensuing targets will be placed in a subdirectory of the ${CMAKE_BINARY_DIR} directory (build directory) whose path mirrors the path of the source itself. To get concrete, this command means that after a successful build we'll have an executable, ./build/appfwk/apps/daq_application , where that path is given relative to the base superproject directory. Without this function call it would end up as ./build/appfwk/daq_application . You'll also notice that we link in the ${DAQ_LIBRARIES_UNIVERSAL_EXE} libraries defined in the superproject CMakeLists.txt, as well as libraries specific to the appfwk package, defined in DAQ_LIBRARIES_PACKAGE The other function currently provided by the DAQ CMake module is add_unit_test . Examples of this function's use can be found at the bottom of the appfwk/CMakeLists.txt file. If you pass this function a name, e.g., mytest , it will create a unit test executable off of a source file called unittest/mytest.cxx , and handle linking in common libraries and the needed Boost unit test dependencies. If you're curious, you can find its implementation in daq-buildtools/CMake/DAQ.cmake , path relative to the superproject base directory. When you write code in CMake good software development principles generally apply. Writing your own macros and functions you can live up to the Don't Repeat Yourself principle ; an example of this can be found in the definition of the MakeDataTypeLibraries macro, which helps avoid boilerplate while instantiating the FanOutDAQModule template class for various types and giving these instantiations well-motivated names. Note that this macro makes use of the configure_file command which lets you modify source files with CMake environment variables. Another thing to be aware of: since you're using a single CMakeLists.txt file, the scope of your decisions can extend to all ensuing code. So, e.g., if you add the line link_libraries( bloated_library_with_extremely_common_symbol_names ) then all ensuing targets from add_executable , add_library , etc., will get that linked in. For this reason, prefer a target-specific call such as target_link_libraries(executable_that_really_needs_this_library bloated_library_with_extremely_common_symbol_names) instead. setup_build_environment As you've discovered from the Compiling-and-Running section you were pointed to at the start of this Wiki, quick-start.sh , along with constructing the superproject CMakeLists.txt file, also constructs a file called setup_build_environment which does what it says it's going to do. It performs ups setups of dependencies (Boost, etc.) which ers and appfwk rely on. If you have any new dependencies, you'll want to add them by looking for the setup_returns variable in setup_build_environment . You'll see there are several packages where for each package, there's a ups setup followed by an appending of the return value of the setup to setup_returns . You should do the same for each dependency you want to add, e.g. setup <dependency_I_am_introducing> <version_of_the_dependency> # And perhaps qualifiers, e.g., -q e19:debug setup_returns=$setup_returns\"$? \" The setup_build_environment script uses setup_returns to check that all the packages were setup without a nonzero return value. You may also want to set up additional environment variables for your new project in setup_build_environment .","title":"Creating a new package"},{"location":"versions/v1.0.0/Creating-a-new-package/#setting-up-a-development-area","text":"To create a new package, you'll want to install a DUNE-DAQ development environment and then create a new CMake project for the package. How to install and build the DUNE-DAQ development environment is described [[here|Compiling-and-running]]. After installation of the development environment, you'll see that there are already two CMake projects installed, appfwk and ers (daq-buildtools, which provides CMake modules for DUNE DAQ developers, is not itself a CMake project). You'll also notice that ers and appfwk are git repos; you'll want your project to reside in a git repo as well.","title":"Setting up a development area"},{"location":"versions/v1.0.0/Creating-a-new-package/#the-superproject-cmakeliststxt","text":"Every project which is installed in your development area is actually a subproject of the dune-day CMake superproject. The superproject's CMakeLists.txt file is constructed when you run quick-start.sh , and resides in the directory out of which you ran it. The CMake code in that file globally influences the CMake code of each project which is a subdirectory; this influence includes: Ensuring that all code is compiled under extension-free, standard C++17 Ensuring that a full set of standard compiler warnings are used Ensuring that the DUNE C++ code linters can work with your C++ code Ensuring that the CMake functions defined in daq-buildtools/CMake/DAQ.cmake are available to your CMake code Ensuring that you won't need to write any CMake code in order for your C++ code to find headers associated with Boost, or the appfwk or ers projects Providing two CMake variables, DAQ_LIBRARIES_UNIVERSAL and DAQ_LIBRARIES_UNIVERSAL_EXE , which define dependencies that any typical DUNE DAQ library ( DAQ_LIBRARIES_UNIVERSAL ) or executable ( DAQ_LIBRARIES_UNIVERSAL_EXE ) should link in You'll also notice that the ers and appfwk projects are added in the superproject CMakeLists.txt via the passing of their namesake subdirectories to CMake's add_subdirectory command; once you have your project subdirectory, you'll want to add it to the superproject CMakeLists.txt file as well so it knows your work should be part of the overall build.","title":"The superproject CMakeLists.txt"},{"location":"versions/v1.0.0/Creating-a-new-package/#your-projects-cmakeliststxt-file","text":"If you're in the directory in which you ran quick-start.sh , you can look at ers/CMakeLists.txt and appfwk/CMakeLists.txt for examples of project CMakeLists.txt files. While a lot can be learned from looking at these examples and reading this documentation, realize that CMake is a full-blown programming language, and like any programming language, the more familiar you are with it the easier life will be. It's extensively documented online; in particular, if you want to learn more about specific CMake functions you can go here for a reference guide of CMake commands, or, if you've sourced setup_build_environment (so CMake is set up), you can even learn at the command line via cmake --help-command <name of command> However, on top of the CMake language itself, there are CMake functions which have been written specifically for DUNE DAQ development, and there's a structure your CMake code should follow; looking at appfwk/CMakeLists.txt is a good way to learn about both of these. Guidelines to follow are: There there should be only one CMakeLists.txt file in your project, and it should be in the base directory of your project. It's recommended that you link the libraries defined by DAQ_LIBRARIES_UNIVERSAL into the libraries you create, and DAQ_LIBRARIES_UNIVERSAL_EXE into the executables you create. An example of the latter is shown in the 'target_link_libraries' line below. You may also want to define a DAQ_LIBRARIES_PACKAGE CMake variable, which define additional libraries your package's executables and libraries should generally depend on CMake commands which are specific to the code in a subdirectory tree should be clustered in the same location, and they should be prefaced with the name of the subdirectory wrapped in the point_build_to CMake function. E.g., for the code in appfwk/apps we have: point_build_to( apps ) add_executable(daq_application apps/daq_application.cxx) target_link_libraries(daq_application ${DAQ_LIBRARIES_UNIVERSAL_EXE} ${DAQ_LIBRARIES_PACKAGE}) Here, point_build_to(apps) will ensure that the ensuing targets will be placed in a subdirectory of the ${CMAKE_BINARY_DIR} directory (build directory) whose path mirrors the path of the source itself. To get concrete, this command means that after a successful build we'll have an executable, ./build/appfwk/apps/daq_application , where that path is given relative to the base superproject directory. Without this function call it would end up as ./build/appfwk/daq_application . You'll also notice that we link in the ${DAQ_LIBRARIES_UNIVERSAL_EXE} libraries defined in the superproject CMakeLists.txt, as well as libraries specific to the appfwk package, defined in DAQ_LIBRARIES_PACKAGE The other function currently provided by the DAQ CMake module is add_unit_test . Examples of this function's use can be found at the bottom of the appfwk/CMakeLists.txt file. If you pass this function a name, e.g., mytest , it will create a unit test executable off of a source file called unittest/mytest.cxx , and handle linking in common libraries and the needed Boost unit test dependencies. If you're curious, you can find its implementation in daq-buildtools/CMake/DAQ.cmake , path relative to the superproject base directory. When you write code in CMake good software development principles generally apply. Writing your own macros and functions you can live up to the Don't Repeat Yourself principle ; an example of this can be found in the definition of the MakeDataTypeLibraries macro, which helps avoid boilerplate while instantiating the FanOutDAQModule template class for various types and giving these instantiations well-motivated names. Note that this macro makes use of the configure_file command which lets you modify source files with CMake environment variables. Another thing to be aware of: since you're using a single CMakeLists.txt file, the scope of your decisions can extend to all ensuing code. So, e.g., if you add the line link_libraries( bloated_library_with_extremely_common_symbol_names ) then all ensuing targets from add_executable , add_library , etc., will get that linked in. For this reason, prefer a target-specific call such as target_link_libraries(executable_that_really_needs_this_library bloated_library_with_extremely_common_symbol_names) instead.","title":"Your project's CMakeLists.txt file"},{"location":"versions/v1.0.0/Creating-a-new-package/#setup_build_environment","text":"As you've discovered from the Compiling-and-Running section you were pointed to at the start of this Wiki, quick-start.sh , along with constructing the superproject CMakeLists.txt file, also constructs a file called setup_build_environment which does what it says it's going to do. It performs ups setups of dependencies (Boost, etc.) which ers and appfwk rely on. If you have any new dependencies, you'll want to add them by looking for the setup_returns variable in setup_build_environment . You'll see there are several packages where for each package, there's a ups setup followed by an appending of the return value of the setup to setup_returns . You should do the same for each dependency you want to add, e.g. setup <dependency_I_am_introducing> <version_of_the_dependency> # And perhaps qualifiers, e.g., -q e19:debug setup_returns=$setup_returns\"$? \" The setup_build_environment script uses setup_returns to check that all the packages were setup without a nonzero return value. You may also want to set up additional environment variables for your new project in setup_build_environment .","title":"setup_build_environment"},{"location":"versions/v1.0.0/Step-by-step-instructions-for-creating-your-own-DAQModule-package/","text":"Introduction This page is intended to provide a step-by-step guide to setting up a work area in which you create software modules that run within Version 1 of the DUNE DAQ software application framework ( appfwk v1). These modules are often called DAQModules since that is the C++ base class that they inherit from. If you are impatient to get started, you can jump to the [[Step-by-step Instructions|Step-by-step-instructions-for-creating-your-own-DAQModule-package-under-v1#step-by-step Installation instructions]] now. New users should probably continue reading this introduction, though, to learn a little bit of background information. This page draws from, and references, several other Wiki pages in the appfwk repository. Links to these pages are listed in the text in the relevant places, and they are a good source of additional information for interested readers. Here are some of these references collected for you in one place on this page: * [[Compiling and running the App Fwk|Compiling-and-running]] * [[Creating a new package|Creating-a-new-package]] and last, but certainly not least * The DUNE DAQ C++ Style Guide This page assumes a basic understanding of how modules are combined to form processes within appfwk v1. A brief recap can be found in the following snippet. A reference for this is the sneak peak talk given by Eric Flumerfelt on 15-Jun-2020. As a refresher: * a *DAQProcess* contains one or more *DAQModules* * when multiple *DAQModules* are present within a *DAQProcess*, they communicate with each other via *Queues* (the diagrams that are included [[later on this page|Step-by-step-instructions-for-creating-your-own-DAQModule-package-under-v1#information-about-the-daqmodules-in-the-listrev-example]] might help explain this) * there are two classes that wrap *Queues* to provide the ability to push data onto the queue (*DAQSink*) or pull data from a queue (*DAQSource*). An individual *DAQModule* will only access one side of each *Queue*. If the module pushes data onto the queue, it will use an instance of the *DAQSink* class (which wraps the desired *Queue*), and if the module pops data from the queue, it will use an instance of the *DAQSource* class. * the *DAQModules* that run within a given process (and the *Queues* between them) are specified in a JSON *process configuration* file. An example of one such file is given below. The creation of the *DAQModules* and *Queues* is handled by the *appfwk*. * at this point in time, we expect that most users will be developing *DAQModules* and simply using existing *Queues* from the *appfwk*. * at the moment, there aren't any centrally-provided libraries, tools, or recommendations for inter-process communication. We expect to address this topic soon, but for now, developers can either focus on single-process examples, or use other software for inter-process communication. The instructions on this page focus on single-process examples. * please remember that there have been a number of compromises or simplifications in the functionality that is provided by *appfwk* v1. We have made a good faith attempt to provide a good start on the internal interfaces and layout of *DAQProcesses*, but things will definitely change over time, and we will be gathering feedback from everyone on ways that things might be improved. (Of course, we already have a fairly good list based on the experience of creating v1.) Step-by-step Installation instructions Here are the suggested steps for setting up a work area, in which you can create your new package: Choose a computer/cluster to work on that has access to /cvmfs/dune.opensciencegrid.org/dunedaq/DUNE/products (more details on compiling and running code with the appfwk are provided on the [[Compiling and Running|Compiling-and-running#if-you-have-access-to-cvmfs-ups-product-areas]] page) this CVMFS area is where several necessary dependent software packages are stored (for now) lxplus.cern.ch has been used successfully for this purpose Log into that computer/cluster, create a fresh directory to work in, and cd into that directory (I'll call this your WORK_DIR .) Run the following four commands (these are based on commands from the [[Compiling and Running|Compiling-and-running#if-you-have-access-to-cvmfs-ups-product-areas]] page) curl -O https://raw.githubusercontent.com/DUNE-DAQ/daq-buildtools/v1.0.0/bin/quick-start.sh chmod +x quick-start.sh sed -r -i 's/edits_check=true/edits_check=false/' quick-start.sh ./quick-start.sh Don't be concerned about the \"User assumes the risk that the script may make out-of-date assumptions\" warning you see. It's because you're not using the newest version of quick-start.sh, you're using the v1.0.0 version. After you've run quick-start.sh we'll account for this by making sure the downloaded repos are pointing to the v1.0.0 codebase: for pkg in appfwk ers daq-buildtools; do cd $pkg; git checkout v1.0.0; cd .. ; done Set up the build environment for this WORK_DIR by running the following command: source ./setup_build_environment Build the core packages that the quick-start.sh sript installed in your working directory by running this command: ./build_daq_software.sh please note that you'll likely see some messages like nlohmann_json NOT FOUND! . This is not a cause for alarm. The build process automatically downloads and builds the nlohmann::json package for you, after it notices that it wasn't found. there may also be a warning message about an unused variable in ers/src/Issue.cxx . Again, this is nothing to worry about. If you would like to try running the example application(s) from the core appfwk repository, you can do that now. There are instructions [[later on this page|Step-by-step-instructions-for-creating-your-own-DAQModule-package#running-the-fanout-example-in-the-appfwk-repo]] for doing that. At this point, you could either check out the example DAQModule package, build it, and run the the example application, or you could start [[creating your own package|Step-by-step-instructions-for-creating-your-own-DAQModule-package#Create-your-own-software-package]]. These instructions will walk you through doing both, but of course you can skip to the latter by scrolling to the dedicated section. Install the example package Here are the steps for adding the appfwk Example package to your work area, building it, and running the example application... (For information on the DAQModules that are contained in the example application and how they interact, please see [[this section|Step-by-step-instructions-for-creating-your-own-DAQModule-package#Information-about-the-DAQModules-in-the-listrev-example]] later on this page.) * cd into your work area directory ( WORK_DIR ), if you aren't there already * run the following command to clone the example package: * git clone https://github.com/DUNE-DAQ/listrev.git * ...and make sure you've got the v1.0.0 version of it * cd listrev * git checkout v1.0.0 * cd .. * add the following line to the bottom of the top-level CMakeLists.txt file (in WORK_DIR ) * add_subdirectory(listrev) * [if not already done in the current shell] set up the build environment for your work area * source ./setup_build_environment * rebuild the software in your work area by running * ./build_daq_software.sh In order to run the example package follow the instruction in the [[dedicated section|Step-by-step-instructions-for-creating-your-own-DAQModule-package-under-v1#Running the example package]]. Create your own software package Here are the commands to create your own software package that depends on appfwk: * create a new directory underneath WORK_DIR (we'll call this YOUR_PKG_DIR ) * copy the CMakeLists.txt file from the example package into YOUR_PKG_DIR (you can fetch it from here ) * edit this CMakeLists.txt file to change all of the instances of the string \"listrev\" to your package name * (we'll do a couple more edits in a bit, but you can save and close this file now * create a src directory underneath YOUR_PKG_DIR * copy one of the DAQModules from the example package into the src directory * as an example, let's copy the RandomDataListGenerator DAQModule from the example package, along with the CommonIssues header file. You can fetch these files from here , here , and here . * change all instances of \"listrev\" and \"LISTREV\" in these files to your package name * edit YOUR_PKG_DIR /CMakeLists.txt to remove the ListReverser and ReversedListValidator add_library and target_link_libraries lines * you should also comment out the point_build_to( test ) and file(COPY test/list_reversal_app.json DESTINATION test) lines * add the following line to the bottom of the top-level CMakeLists.txt file (in WORK_DIR ) * add_subdirectory(YOUR_PKG_DIR) * cd to WORK_DIR * [if not already done in the current shell] set up the build environment for your work area * source ./setup_build_environment * rebuild the software in your work area by running * ./build_daq_software.sh * at this point, you'll have a decent start on your own DAQModule package. Of course, you'll need to rename and modify the RandomDataListGenerator DAQModule to do whatever you want your first DAQModule to do. And, when you get the point of running your DAQProcess that uses your DAQModule (s), you'll need to create a JSON process configuration file to use for that, but hopefully the listrev example package will help. How to write a DAQModule From the C++ point of view, DAQ modules are implementation of a [[DAQ Module interface|DAQModules]]. That means that apart from the constructor (that receives a name) only one method has to be implemented: init() . The function has to handle all those configuration items that are not supposed to be reconfigurable during the run. Optionally it can configure all those variables that can be overridden during run time. The most important thing that init has to do is to call register_command in order to associate functions to each command that the module is expected to support. Each function has to be a set of operations that changes the internal status of the module according to the command that it is implementing. The prototype of each function is in the form void do_something( const vector<string> & args ) ; The function can be virtual and the system is able to pick the most downstream specification. Assuming that the name of the module you are developing is MyNewDAQModule a call to associate the command \"start\" to a function called do_start will be register_command(\"start\", & MyNewDAQModule::do_start); It is expected that the operations of the DAQ Module are carried on in other threads under the control of the DAQModule itself. How to make your DAQModule a plugin of the appfmw DAQModules need to be created with a factory pattern simply via their name. In the application framework this is done using CETlib . In order to do that, each implementation ( cpp file) needs to have a call to the macro DEFINE_DUNE_DAQ_MODULE(<name_including_namespace>) for example DEFINE_DUNE_DAQ_MODULE(dunedaq::appfwk::DummyModule) In order to generate a shared object library that is linkable in runtime from the application framework, the name of the library has to be the in the form <package>_<module_name_no_namespace>_duneDAQModule . This can be achieved simply adding a line in the CMakeLists.txt file of your project in the form add_library(<package>_<module_name_no_namespace>_duneDAQModule path/to/my/file.cpp) . For example: add_library(appfwk_DummyModule_duneDAQModule test/DummyModule.cpp) Running examples Running the example package Assuming you installed your software as described in the previous sections, these are the instructions to run the code: * set up the runtime environment setup script by running this command: * source ./setup_runtime_environment * please note that this needs to be done from WORK_DIR * run the example application using the following command: * build/appfwk/apps/daq_application -c QueryResponseCommandFacility -j build/listrev/test/list_reversal_app.json * once the program is running, and you see the \"Enter a command\" prompt, you can type in commands like the following: * configure * This command pretends to set the values of configurable parameters like the number of integers in each randomly generated list * start * This command is passed to the three DAQModules in a well-specified order (specified in the JSON process configuration file). First, the validator is started, then the reverser , then the generator . Once the generator is started, it begins creating lists of integers and passing them to the other two DAQModules . * On the console, you will see ERS LOG messages from each of the DAQModules saying that they started (so you can confirm that the start order is correct), and then you will see ERS DEBUG messages that tell you what each of the DAQModules is doing as they process the lists of integers. * stop * This command stops the three DAQModules , in the reverse order that they were started (like the start order, the stop order is specified in the JSON process config file). * You will need to type this command into the console a little blindly, since the ERS DEBUG messages will be printing to the console as the program runs. * After each of the three DAQModules has finished, they print an ERS INFO message to the console with a summary of what they accomplished. * unconfigure * This command pretends to tear-down whatever configuration was established in the configure step. * quit * This command exits the program. Information about the DAQModules in the listrev example The idea behind the listrev example is to have one DAQModule that generates a list of random integers, another DAQModule that reverses a copy of the list, and a third DAQModule that compares copies of the original and reversed lists to validate that they are equivalent, modulo the reversal. This is shown in the following diagrams. The first one provides a little description of what each of the DAQModules is doing, and the second one shows the class names of the DAQModules and the configured names of the DAQModules and the Queues , as they are identified in the JSON process configuration file. A copy of the listrev JSON process configuration file is shown below (the official copy of this file in the repo is here ). As you can see, the Queues are specified first, then the DAQModules , and the DAQModules include the configuration of which Queues they make use of. * The capacity and kind parameters within the queue declarations are required. The names of the queues (e.g. primaryDataQueue ) are your choice. * The user_module_type parameter in the module declarations is required. The names of the parameters that specify the queues to the modules are up to you. As you can see in this example, different parameter names are used to specify the queues to the three different modules. (Of course, what the DAQModule code does with the queue names is standardized. An example of that is shown in this snippet of listrev code .) { \"queues\": { \"primaryDataQueue\": { \"capacity\": 10, \"kind\": \"FollySPSCQueue\" }, \"reversedDataQueue\": { \"capacity\": 10, \"kind\": \"FollySPSCQueue\" }, \"dataCopyQueue\": { \"capacity\": 10, \"kind\": \"FollySPSCQueue\" } }, \"modules\": { \"generator\": { \"user_module_type\": \"RandomDataListGenerator\", \"outputs\": [ \"primaryDataQueue\", \"dataCopyQueue\" ] }, \"reverser\": { \"user_module_type\": \"ListReverser\", \"input\": \"primaryDataQueue\", \"output\": \"reversedDataQueue\" }, \"validator\": { \"user_module_type\": \"ReversedListValidator\", \"reversed_data_input\": \"reversedDataQueue\", \"original_data_input\": \"dataCopyQueue\" } }, \"commands\": { \"start\": [ \"validator\", \"reverser\", \"generator\" ], \"stop\": [ \"generator\", \"reverser\", \"validator\" ] } } As mentioned earlier on this page, when you run the listrev example, you will see ERS messages printed out in the console. ERS is the Error Reporting Service from the ATLAS experiment. Within the DUNE DAQ, we have our own fork of that package, and you will see a clone of the DUNE DAQ ERS repo when you look at the directories underneath WORK_DIR . For further information on ERS, you can see this journal article (PDF) . In addition to ERS messages, there are TRACE messages in the listrev example code. The current model for using ERS and TRACE is described in a section of the Style Guide , and the example code follows those guidelines. For example, the example DAQModule s usee ERS Issues for warnings, errors, and fatal errors, and TRACE messages for messages that developers would use for debugging or verifying the behavior of the code. The periodic progress reports are implemented as ERS DEBUG messages, although admittedly, they could just as easily been implemented as TRACE messages. The choice there was simply a practical one - that ERS messages are displayed on the console by default, whereas TRACE messages typically go to a memory area by default so that they use very little system resources. (TRACE messages can trivially be directed to the console, but that would have been just one more step in the instructions above.) Users who are interested in seeing the TRACE messages from the listrev example code, or their own DAQModules when the time comes, can use the following steps: * before running the program, set the TRACE_FILE environmental variable to point to a file underneath your WORK_DIR * export TRACE_FILE=<WORK_DIR>/log/${USER}_dunedaq.trace * run the program * look at the TRACE levels that are enabled for each TRACE_NAME (TRACE_NAMEs are used to help identify which source file the messages were sent from) * tlvls * enable the TRACE levels that you would like to see appear in the TRACE memory buffer with commands like the following: * tonM -n RandomDataListGenerator 10 * tonM -n RandomDataListGenerator 15 * view the messages in the TRACE memory buffer. I appreciate seeing the timestamps in human-readable form, so I typically pipe the output of tshow to tdelta as shown here (both are provided by the TRACE package) * tshow | tdelta -ct 1 | more * Note that the messages from the TRACE buffer are displayed in reverse time order (most recent message first) This short introduction to TRACE only describes a small fraction of its capabilities, and interested users are encouraged to read the Quick Start guide, the User's Guide, and other documentation provided here . Running the Fanout example in the appfwk repo In a fresh shell, here are the steps that you would use to run the Fanout example in the appfwk repo after you have run quick-start.sh and built the software using build_daq_software.sh : * cd to your WORK_DIR * run the following commands to set up the build and runtime environments: * source ./setup_build_environment * source ./setup_runtime_environment * run the following command to start the example: * build/appfwk/apps/daq_application -c QueryResponseCommandFacility -j appfwk/test/producer_consumer_dynamic_test.json * enter commands like the following: * configure * start * stop * quit","title":"Introduction"},{"location":"versions/v1.0.0/Step-by-step-instructions-for-creating-your-own-DAQModule-package/#introduction","text":"This page is intended to provide a step-by-step guide to setting up a work area in which you create software modules that run within Version 1 of the DUNE DAQ software application framework ( appfwk v1). These modules are often called DAQModules since that is the C++ base class that they inherit from. If you are impatient to get started, you can jump to the [[Step-by-step Instructions|Step-by-step-instructions-for-creating-your-own-DAQModule-package-under-v1#step-by-step Installation instructions]] now. New users should probably continue reading this introduction, though, to learn a little bit of background information. This page draws from, and references, several other Wiki pages in the appfwk repository. Links to these pages are listed in the text in the relevant places, and they are a good source of additional information for interested readers. Here are some of these references collected for you in one place on this page: * [[Compiling and running the App Fwk|Compiling-and-running]] * [[Creating a new package|Creating-a-new-package]] and last, but certainly not least * The DUNE DAQ C++ Style Guide This page assumes a basic understanding of how modules are combined to form processes within appfwk v1. A brief recap can be found in the following snippet. A reference for this is the sneak peak talk given by Eric Flumerfelt on 15-Jun-2020. As a refresher: * a *DAQProcess* contains one or more *DAQModules* * when multiple *DAQModules* are present within a *DAQProcess*, they communicate with each other via *Queues* (the diagrams that are included [[later on this page|Step-by-step-instructions-for-creating-your-own-DAQModule-package-under-v1#information-about-the-daqmodules-in-the-listrev-example]] might help explain this) * there are two classes that wrap *Queues* to provide the ability to push data onto the queue (*DAQSink*) or pull data from a queue (*DAQSource*). An individual *DAQModule* will only access one side of each *Queue*. If the module pushes data onto the queue, it will use an instance of the *DAQSink* class (which wraps the desired *Queue*), and if the module pops data from the queue, it will use an instance of the *DAQSource* class. * the *DAQModules* that run within a given process (and the *Queues* between them) are specified in a JSON *process configuration* file. An example of one such file is given below. The creation of the *DAQModules* and *Queues* is handled by the *appfwk*. * at this point in time, we expect that most users will be developing *DAQModules* and simply using existing *Queues* from the *appfwk*. * at the moment, there aren't any centrally-provided libraries, tools, or recommendations for inter-process communication. We expect to address this topic soon, but for now, developers can either focus on single-process examples, or use other software for inter-process communication. The instructions on this page focus on single-process examples. * please remember that there have been a number of compromises or simplifications in the functionality that is provided by *appfwk* v1. We have made a good faith attempt to provide a good start on the internal interfaces and layout of *DAQProcesses*, but things will definitely change over time, and we will be gathering feedback from everyone on ways that things might be improved. (Of course, we already have a fairly good list based on the experience of creating v1.)","title":"Introduction"},{"location":"versions/v1.0.0/Step-by-step-instructions-for-creating-your-own-DAQModule-package/#step-by-step-installation-instructions","text":"Here are the suggested steps for setting up a work area, in which you can create your new package: Choose a computer/cluster to work on that has access to /cvmfs/dune.opensciencegrid.org/dunedaq/DUNE/products (more details on compiling and running code with the appfwk are provided on the [[Compiling and Running|Compiling-and-running#if-you-have-access-to-cvmfs-ups-product-areas]] page) this CVMFS area is where several necessary dependent software packages are stored (for now) lxplus.cern.ch has been used successfully for this purpose Log into that computer/cluster, create a fresh directory to work in, and cd into that directory (I'll call this your WORK_DIR .) Run the following four commands (these are based on commands from the [[Compiling and Running|Compiling-and-running#if-you-have-access-to-cvmfs-ups-product-areas]] page) curl -O https://raw.githubusercontent.com/DUNE-DAQ/daq-buildtools/v1.0.0/bin/quick-start.sh chmod +x quick-start.sh sed -r -i 's/edits_check=true/edits_check=false/' quick-start.sh ./quick-start.sh Don't be concerned about the \"User assumes the risk that the script may make out-of-date assumptions\" warning you see. It's because you're not using the newest version of quick-start.sh, you're using the v1.0.0 version. After you've run quick-start.sh we'll account for this by making sure the downloaded repos are pointing to the v1.0.0 codebase: for pkg in appfwk ers daq-buildtools; do cd $pkg; git checkout v1.0.0; cd .. ; done Set up the build environment for this WORK_DIR by running the following command: source ./setup_build_environment Build the core packages that the quick-start.sh sript installed in your working directory by running this command: ./build_daq_software.sh please note that you'll likely see some messages like nlohmann_json NOT FOUND! . This is not a cause for alarm. The build process automatically downloads and builds the nlohmann::json package for you, after it notices that it wasn't found. there may also be a warning message about an unused variable in ers/src/Issue.cxx . Again, this is nothing to worry about. If you would like to try running the example application(s) from the core appfwk repository, you can do that now. There are instructions [[later on this page|Step-by-step-instructions-for-creating-your-own-DAQModule-package#running-the-fanout-example-in-the-appfwk-repo]] for doing that. At this point, you could either check out the example DAQModule package, build it, and run the the example application, or you could start [[creating your own package|Step-by-step-instructions-for-creating-your-own-DAQModule-package#Create-your-own-software-package]]. These instructions will walk you through doing both, but of course you can skip to the latter by scrolling to the dedicated section.","title":"Step-by-step Installation instructions"},{"location":"versions/v1.0.0/Step-by-step-instructions-for-creating-your-own-DAQModule-package/#install-the-example-package","text":"Here are the steps for adding the appfwk Example package to your work area, building it, and running the example application... (For information on the DAQModules that are contained in the example application and how they interact, please see [[this section|Step-by-step-instructions-for-creating-your-own-DAQModule-package#Information-about-the-DAQModules-in-the-listrev-example]] later on this page.) * cd into your work area directory ( WORK_DIR ), if you aren't there already * run the following command to clone the example package: * git clone https://github.com/DUNE-DAQ/listrev.git * ...and make sure you've got the v1.0.0 version of it * cd listrev * git checkout v1.0.0 * cd .. * add the following line to the bottom of the top-level CMakeLists.txt file (in WORK_DIR ) * add_subdirectory(listrev) * [if not already done in the current shell] set up the build environment for your work area * source ./setup_build_environment * rebuild the software in your work area by running * ./build_daq_software.sh In order to run the example package follow the instruction in the [[dedicated section|Step-by-step-instructions-for-creating-your-own-DAQModule-package-under-v1#Running the example package]].","title":"Install the example package"},{"location":"versions/v1.0.0/Step-by-step-instructions-for-creating-your-own-DAQModule-package/#create-your-own-software-package","text":"Here are the commands to create your own software package that depends on appfwk: * create a new directory underneath WORK_DIR (we'll call this YOUR_PKG_DIR ) * copy the CMakeLists.txt file from the example package into YOUR_PKG_DIR (you can fetch it from here ) * edit this CMakeLists.txt file to change all of the instances of the string \"listrev\" to your package name * (we'll do a couple more edits in a bit, but you can save and close this file now * create a src directory underneath YOUR_PKG_DIR * copy one of the DAQModules from the example package into the src directory * as an example, let's copy the RandomDataListGenerator DAQModule from the example package, along with the CommonIssues header file. You can fetch these files from here , here , and here . * change all instances of \"listrev\" and \"LISTREV\" in these files to your package name * edit YOUR_PKG_DIR /CMakeLists.txt to remove the ListReverser and ReversedListValidator add_library and target_link_libraries lines * you should also comment out the point_build_to( test ) and file(COPY test/list_reversal_app.json DESTINATION test) lines * add the following line to the bottom of the top-level CMakeLists.txt file (in WORK_DIR ) * add_subdirectory(YOUR_PKG_DIR) * cd to WORK_DIR * [if not already done in the current shell] set up the build environment for your work area * source ./setup_build_environment * rebuild the software in your work area by running * ./build_daq_software.sh * at this point, you'll have a decent start on your own DAQModule package. Of course, you'll need to rename and modify the RandomDataListGenerator DAQModule to do whatever you want your first DAQModule to do. And, when you get the point of running your DAQProcess that uses your DAQModule (s), you'll need to create a JSON process configuration file to use for that, but hopefully the listrev example package will help.","title":"Create your own software package"},{"location":"versions/v1.0.0/Step-by-step-instructions-for-creating-your-own-DAQModule-package/#how-to-write-a-daqmodule","text":"From the C++ point of view, DAQ modules are implementation of a [[DAQ Module interface|DAQModules]]. That means that apart from the constructor (that receives a name) only one method has to be implemented: init() . The function has to handle all those configuration items that are not supposed to be reconfigurable during the run. Optionally it can configure all those variables that can be overridden during run time. The most important thing that init has to do is to call register_command in order to associate functions to each command that the module is expected to support. Each function has to be a set of operations that changes the internal status of the module according to the command that it is implementing. The prototype of each function is in the form void do_something( const vector<string> & args ) ; The function can be virtual and the system is able to pick the most downstream specification. Assuming that the name of the module you are developing is MyNewDAQModule a call to associate the command \"start\" to a function called do_start will be register_command(\"start\", & MyNewDAQModule::do_start); It is expected that the operations of the DAQ Module are carried on in other threads under the control of the DAQModule itself.","title":"How to write a DAQModule"},{"location":"versions/v1.0.0/Step-by-step-instructions-for-creating-your-own-DAQModule-package/#how-to-make-your-daqmodule-a-plugin-of-the-appfmw","text":"DAQModules need to be created with a factory pattern simply via their name. In the application framework this is done using CETlib . In order to do that, each implementation ( cpp file) needs to have a call to the macro DEFINE_DUNE_DAQ_MODULE(<name_including_namespace>) for example DEFINE_DUNE_DAQ_MODULE(dunedaq::appfwk::DummyModule) In order to generate a shared object library that is linkable in runtime from the application framework, the name of the library has to be the in the form <package>_<module_name_no_namespace>_duneDAQModule . This can be achieved simply adding a line in the CMakeLists.txt file of your project in the form add_library(<package>_<module_name_no_namespace>_duneDAQModule path/to/my/file.cpp) . For example: add_library(appfwk_DummyModule_duneDAQModule test/DummyModule.cpp)","title":"How to make your DAQModule a plugin of the appfmw"},{"location":"versions/v1.0.0/Step-by-step-instructions-for-creating-your-own-DAQModule-package/#running-examples","text":"","title":"Running examples"},{"location":"versions/v1.0.0/Step-by-step-instructions-for-creating-your-own-DAQModule-package/#running-the-example-package","text":"Assuming you installed your software as described in the previous sections, these are the instructions to run the code: * set up the runtime environment setup script by running this command: * source ./setup_runtime_environment * please note that this needs to be done from WORK_DIR * run the example application using the following command: * build/appfwk/apps/daq_application -c QueryResponseCommandFacility -j build/listrev/test/list_reversal_app.json * once the program is running, and you see the \"Enter a command\" prompt, you can type in commands like the following: * configure * This command pretends to set the values of configurable parameters like the number of integers in each randomly generated list * start * This command is passed to the three DAQModules in a well-specified order (specified in the JSON process configuration file). First, the validator is started, then the reverser , then the generator . Once the generator is started, it begins creating lists of integers and passing them to the other two DAQModules . * On the console, you will see ERS LOG messages from each of the DAQModules saying that they started (so you can confirm that the start order is correct), and then you will see ERS DEBUG messages that tell you what each of the DAQModules is doing as they process the lists of integers. * stop * This command stops the three DAQModules , in the reverse order that they were started (like the start order, the stop order is specified in the JSON process config file). * You will need to type this command into the console a little blindly, since the ERS DEBUG messages will be printing to the console as the program runs. * After each of the three DAQModules has finished, they print an ERS INFO message to the console with a summary of what they accomplished. * unconfigure * This command pretends to tear-down whatever configuration was established in the configure step. * quit * This command exits the program.","title":"Running the example package"},{"location":"versions/v1.0.0/Step-by-step-instructions-for-creating-your-own-DAQModule-package/#information-about-the-daqmodules-in-the-listrev-example","text":"The idea behind the listrev example is to have one DAQModule that generates a list of random integers, another DAQModule that reverses a copy of the list, and a third DAQModule that compares copies of the original and reversed lists to validate that they are equivalent, modulo the reversal. This is shown in the following diagrams. The first one provides a little description of what each of the DAQModules is doing, and the second one shows the class names of the DAQModules and the configured names of the DAQModules and the Queues , as they are identified in the JSON process configuration file. A copy of the listrev JSON process configuration file is shown below (the official copy of this file in the repo is here ). As you can see, the Queues are specified first, then the DAQModules , and the DAQModules include the configuration of which Queues they make use of. * The capacity and kind parameters within the queue declarations are required. The names of the queues (e.g. primaryDataQueue ) are your choice. * The user_module_type parameter in the module declarations is required. The names of the parameters that specify the queues to the modules are up to you. As you can see in this example, different parameter names are used to specify the queues to the three different modules. (Of course, what the DAQModule code does with the queue names is standardized. An example of that is shown in this snippet of listrev code .) { \"queues\": { \"primaryDataQueue\": { \"capacity\": 10, \"kind\": \"FollySPSCQueue\" }, \"reversedDataQueue\": { \"capacity\": 10, \"kind\": \"FollySPSCQueue\" }, \"dataCopyQueue\": { \"capacity\": 10, \"kind\": \"FollySPSCQueue\" } }, \"modules\": { \"generator\": { \"user_module_type\": \"RandomDataListGenerator\", \"outputs\": [ \"primaryDataQueue\", \"dataCopyQueue\" ] }, \"reverser\": { \"user_module_type\": \"ListReverser\", \"input\": \"primaryDataQueue\", \"output\": \"reversedDataQueue\" }, \"validator\": { \"user_module_type\": \"ReversedListValidator\", \"reversed_data_input\": \"reversedDataQueue\", \"original_data_input\": \"dataCopyQueue\" } }, \"commands\": { \"start\": [ \"validator\", \"reverser\", \"generator\" ], \"stop\": [ \"generator\", \"reverser\", \"validator\" ] } } As mentioned earlier on this page, when you run the listrev example, you will see ERS messages printed out in the console. ERS is the Error Reporting Service from the ATLAS experiment. Within the DUNE DAQ, we have our own fork of that package, and you will see a clone of the DUNE DAQ ERS repo when you look at the directories underneath WORK_DIR . For further information on ERS, you can see this journal article (PDF) . In addition to ERS messages, there are TRACE messages in the listrev example code. The current model for using ERS and TRACE is described in a section of the Style Guide , and the example code follows those guidelines. For example, the example DAQModule s usee ERS Issues for warnings, errors, and fatal errors, and TRACE messages for messages that developers would use for debugging or verifying the behavior of the code. The periodic progress reports are implemented as ERS DEBUG messages, although admittedly, they could just as easily been implemented as TRACE messages. The choice there was simply a practical one - that ERS messages are displayed on the console by default, whereas TRACE messages typically go to a memory area by default so that they use very little system resources. (TRACE messages can trivially be directed to the console, but that would have been just one more step in the instructions above.) Users who are interested in seeing the TRACE messages from the listrev example code, or their own DAQModules when the time comes, can use the following steps: * before running the program, set the TRACE_FILE environmental variable to point to a file underneath your WORK_DIR * export TRACE_FILE=<WORK_DIR>/log/${USER}_dunedaq.trace * run the program * look at the TRACE levels that are enabled for each TRACE_NAME (TRACE_NAMEs are used to help identify which source file the messages were sent from) * tlvls * enable the TRACE levels that you would like to see appear in the TRACE memory buffer with commands like the following: * tonM -n RandomDataListGenerator 10 * tonM -n RandomDataListGenerator 15 * view the messages in the TRACE memory buffer. I appreciate seeing the timestamps in human-readable form, so I typically pipe the output of tshow to tdelta as shown here (both are provided by the TRACE package) * tshow | tdelta -ct 1 | more * Note that the messages from the TRACE buffer are displayed in reverse time order (most recent message first) This short introduction to TRACE only describes a small fraction of its capabilities, and interested users are encouraged to read the Quick Start guide, the User's Guide, and other documentation provided here .","title":"Information about the DAQModules in the listrev example"},{"location":"versions/v1.0.0/Step-by-step-instructions-for-creating-your-own-DAQModule-package/#running-the-fanout-example-in-the-appfwk-repo","text":"In a fresh shell, here are the steps that you would use to run the Fanout example in the appfwk repo after you have run quick-start.sh and built the software using build_daq_software.sh : * cd to your WORK_DIR * run the following commands to set up the build and runtime environments: * source ./setup_build_environment * source ./setup_runtime_environment * run the following command to start the example: * build/appfwk/apps/daq_application -c QueryResponseCommandFacility -j appfwk/test/producer_consumer_dynamic_test.json * enter commands like the following: * configure * start * stop * quit","title":"Running the Fanout example in the appfwk repo"},{"location":"versions/v1.0.0/Testing/","text":"Broadly speaking, there are two types of test program you can run: unit tests, and integration tests. Unit tests Unit tests are standalone programs with names of the form <code unit under test>_test and are implemented in the ./unittest subdirectory of a package. They're written using the Boost.Test library. For complete documentation on the Boost.Test library, you can look at the Boost documentation ; for substantially quicker but still useful intros, take a look here and here . While you can run a unit test program without arguments, some arguments are very useful - in particular, those that control which test cases are run, and those that control the level of logging output. To maximize the output to screen, add the argument -l all , and to minimize it, add -l nothing . Other intermediate levels are covered in the Boost documentation. To skip a test, you can add an argument of the form --run_test=\\!test_you_dont_want ; note the exclamation point needs to be escaped with a backslash since the shell interprets it as a special character otherwise. Integration tests Integration tests look at how various units of code interact; unlike unit tests, there's no common approach to running them. As such, a list of integration tests and how to run them can be found in the list below: [[Queue testing|Queue-testing]] dummy_test_app: A simple application that statically instantiates a DummyModule DAQModule, which simply takes the command \"stuff\" to echo a string producer_consumer_dynamic_test.json: A sample configuration file for daq_application which runs a version of the [[Simple readout application|Simple-readout-application]]","title":"Testing"},{"location":"versions/v1.0.0/Testing/#unit-tests","text":"Unit tests are standalone programs with names of the form <code unit under test>_test and are implemented in the ./unittest subdirectory of a package. They're written using the Boost.Test library. For complete documentation on the Boost.Test library, you can look at the Boost documentation ; for substantially quicker but still useful intros, take a look here and here . While you can run a unit test program without arguments, some arguments are very useful - in particular, those that control which test cases are run, and those that control the level of logging output. To maximize the output to screen, add the argument -l all , and to minimize it, add -l nothing . Other intermediate levels are covered in the Boost documentation. To skip a test, you can add an argument of the form --run_test=\\!test_you_dont_want ; note the exclamation point needs to be escaped with a backslash since the shell interprets it as a special character otherwise.","title":"Unit tests"},{"location":"versions/v1.0.0/Testing/#integration-tests","text":"Integration tests look at how various units of code interact; unlike unit tests, there's no common approach to running them. As such, a list of integration tests and how to run them can be found in the list below: [[Queue testing|Queue-testing]] dummy_test_app: A simple application that statically instantiates a DummyModule DAQModule, which simply takes the command \"stuff\" to echo a string producer_consumer_dynamic_test.json: A sample configuration file for daq_application which runs a version of the [[Simple readout application|Simple-readout-application]]","title":"Integration tests"},{"location":"versions/v1.1.0/Compiling-and-running/","text":"To get set up, you'll need access to the ups product area /cvmfs/dune.opensciencegrid.org/dunedaq/DUNE/products , as is the case, e.g., on the lxplus machines at CERN. If you're on a system which has access to this product area, simply do the following after you've logged in to your system and created an empty directory (we'll refer to it as \"MyTopDir\" on this wiki): curl -O https://raw.githubusercontent.com/DUNE-DAQ/daq-buildtools/v1.1.0/bin/quick-start.sh sed -i -r 's/edits_check=true/edits_check=false/g' quick-start.sh # Since we're not using quick-start.sh from the head of develop chmod +x quick-start.sh ./quick-start.sh which will install the needed repos and provide you with a file you can then source to set up the build environment, setup_build_environment , and an executable script you can run for builds, build_daq_software.sh . Disregard the warning concerning the script's self-check being turned off; the default behavior we've switched off is that quick-start.sh attempts to ensure that it's same as the quick-start.sh at the head of develop in the central repo. If, after running quick-start.sh , you want to build the appfwk package and install it in a local subdirectory called ./install right away, all you need to do is the following: . ./setup_build_environment # Only needs to be done once in a given shell ./build_daq_software.sh --install build_daq_software.sh will by default skip CMake's config+generate stages and go straight to the build stage unless the CMakeCache.txt file isn't found in ./build/appfwk -- which is of course the case the first time you run it. If you want to remove all the contents of ./build/appfwk and run config+generate+build, all you need to do is add the --clean option, i.e. ./build_daq_software.sh --clean --install And if, after the build, you want to run the unit tests, just add the --unittest option. Note that it can be used with or without --clean , so, e.g.: ./build_daq_software.sh --clean --install --unittest # Blow away the contents of ./build/appfwk, run config+generate+build, and then run the unit tests ..where in the above case, you blow away the contents of ./build/appfwk, run config+generate+build, install the result in ./install and then run the unit tests. If you want to develop a package other than appfwk, you'll want to add the --pkgname <package name> option. E.g., if we want to build listrev (a simple DAQModule package written by Kurt Biery and documented elsewhere on this wiki), from MyTopDir we can do the following: git clone https://github.com/DUNE-DAQ/listrev.git cd listrev git checkout v1.1.0 cd .. ./build_daq_software.sh --install --pkgname listrev ...where here it's assumed you've already built and locally installed appfwk since it's a dependency of listrev. If you want to see verbose output from the compiler, all you need to do is add the --verbose option: ./build_daq_software.sh --verbose --pkgname <package whose build you're troubleshooting> You can see all the options listed if you run the script with the --help command, i.e. ./build_daq_software.sh --help Finally, note that both the output of your builds and your unit tests are logged to files in the ./log subdirectory. These files may have ASCII color codes which make them difficult to read with some tools; more or cat , however, will display the colors and not the codes themselves. Running In order to run the applications built during the above procedure, the system needs to be instructed on where to look for the libraries that can be used to instantiate objects. In order to do that, the environmental variable CET_PLUGIN_PATH has to contain the path to your compiled src and test sub-directories. This is handled by the ./setup_runtime_environment script which was placed in MyTopDir when you ran quick-start.sh; all you need to do is source it: . ./setup_runtime_environment Note that if you add a new repo to your development area, after building your new code you'll need to source the script again. Once the runtime environment is set, just run the application you need. For example, from MyTopDir, you can run daq_application -c QueryResponseCommandFacility -j appfwk/test/producer_consumer_dynamic_test.json This example accepts commands configure , start , stop , unconfigure , quit . daq_application Command Line Arguments Use `daq_application --help` to see all of the possible options: $ ./build/appfwk/apps/daq_application --help ./build/appfwk/apps/daq_application known arguments (additional arguments will be stored and passed on): -c [ --commandFacility ] arg CommandFacility plugin name -m [ --configManager ] arg ConfigurationManager plugin name -s [ --service ] arg Service plugin(s) to load -j [ --configJson ] arg JSON Application configuration file name -h [ --help ] produce help message Some additional information ### TRACE Messages To enable the sending of TRACE messages to a memory buffer, you can set one of several TRACE environmental variables _before_ running `appfwk/apps/simple_test_app`. One example is to use a command like `export TRACE_NAME=TRACE`. (For more details, please see the [TRACE package documentation](https://cdcvs.fnal.gov/redmine/projects/trace/wiki/Wiki). For example, the [Circular Memory Buffer](https://cdcvs.fnal.gov/redmine/projects/trace/wiki/Circular_Memory_Buffer) section in the TRACE Quick Start talks about the env vars that you can use to enable tracing.) To view the TRACE messages in the memory buffer, you can use the following additional steps: * [if not done already] `export SPACK_ROOT= ; source $SPACK_ROOT/setup-env.sh` * [if not done already] `spack load trace` * `trace_cntl show` or `trace_cntl show | trace_delta -ct 1` (The latter displays the timestamps in human-readable format. Note that the messages are listed in reverse chronological order in both cases.) Testing and running inside a docker container Requirements on the host machine: * installation of cvmfs (instructions can be found here ); * installation of docker (instructions are available on its official website). Docker image recommended to use is dingpf/artdaq:latest . Its corresponding dockerfile can be found in this repo . Use docker run --rm -it -v /cvmfs:/cvmfs -v $PWD:/scratch dingpf/artdaq to start the container, where it mounts your local working directory as /scratch inside the container. You can then treat /scratch as the empty MyTopDir directory referred to in the instructions at the top of this page. Next Steps [[Creating a new package under v1.1.0|Creating-a-new-package-under-v1.1.0]] [[Step-by-step instructions for creating your own DAQModule package under v1.1.0|Step-by-step-instructions-for-creating-your-own-DAQModule-package-under-v1.1.0]]","title":"Compiling and running"},{"location":"versions/v1.1.0/Compiling-and-running/#running","text":"In order to run the applications built during the above procedure, the system needs to be instructed on where to look for the libraries that can be used to instantiate objects. In order to do that, the environmental variable CET_PLUGIN_PATH has to contain the path to your compiled src and test sub-directories. This is handled by the ./setup_runtime_environment script which was placed in MyTopDir when you ran quick-start.sh; all you need to do is source it: . ./setup_runtime_environment Note that if you add a new repo to your development area, after building your new code you'll need to source the script again. Once the runtime environment is set, just run the application you need. For example, from MyTopDir, you can run daq_application -c QueryResponseCommandFacility -j appfwk/test/producer_consumer_dynamic_test.json This example accepts commands configure , start , stop , unconfigure , quit . daq_application Command Line Arguments Use `daq_application --help` to see all of the possible options: $ ./build/appfwk/apps/daq_application --help ./build/appfwk/apps/daq_application known arguments (additional arguments will be stored and passed on): -c [ --commandFacility ] arg CommandFacility plugin name -m [ --configManager ] arg ConfigurationManager plugin name -s [ --service ] arg Service plugin(s) to load -j [ --configJson ] arg JSON Application configuration file name -h [ --help ] produce help message Some additional information ### TRACE Messages To enable the sending of TRACE messages to a memory buffer, you can set one of several TRACE environmental variables _before_ running `appfwk/apps/simple_test_app`. One example is to use a command like `export TRACE_NAME=TRACE`. (For more details, please see the [TRACE package documentation](https://cdcvs.fnal.gov/redmine/projects/trace/wiki/Wiki). For example, the [Circular Memory Buffer](https://cdcvs.fnal.gov/redmine/projects/trace/wiki/Circular_Memory_Buffer) section in the TRACE Quick Start talks about the env vars that you can use to enable tracing.) To view the TRACE messages in the memory buffer, you can use the following additional steps: * [if not done already] `export SPACK_ROOT= ; source $SPACK_ROOT/setup-env.sh` * [if not done already] `spack load trace` * `trace_cntl show` or `trace_cntl show | trace_delta -ct 1` (The latter displays the timestamps in human-readable format. Note that the messages are listed in reverse chronological order in both cases.)","title":"Running"},{"location":"versions/v1.1.0/Compiling-and-running/#testing-and-running-inside-a-docker-container","text":"Requirements on the host machine: * installation of cvmfs (instructions can be found here ); * installation of docker (instructions are available on its official website). Docker image recommended to use is dingpf/artdaq:latest . Its corresponding dockerfile can be found in this repo . Use docker run --rm -it -v /cvmfs:/cvmfs -v $PWD:/scratch dingpf/artdaq to start the container, where it mounts your local working directory as /scratch inside the container. You can then treat /scratch as the empty MyTopDir directory referred to in the instructions at the top of this page.","title":"Testing and running inside a docker container"},{"location":"versions/v1.1.0/Compiling-and-running/#next-steps","text":"[[Creating a new package under v1.1.0|Creating-a-new-package-under-v1.1.0]] [[Step-by-step instructions for creating your own DAQModule package under v1.1.0|Step-by-step-instructions-for-creating-your-own-DAQModule-package-under-v1.1.0]]","title":"Next Steps"},{"location":"versions/v1.1.0/Creating-a-new-package/","text":"Setting up a development area To create a new package, you'll want to install a DUNE-DAQ development environment and then create a new CMake project for the package. How to install and build the DUNE-DAQ development environment is described [[here|Compiling-and-running-under-v1.1.0]]. By the time you've set up the development environment as described in the linked instructions, you'll have already downloaded, built, and locally installed the CMake project for appfwk. You'll likely also have noticed that ./appfwk is a git repo; you'll want your project to reside in a git repo as well. A package's subdirectory structure To learn a bit more about how to structure your package so that it can be incorporated into the DUNE DAQ software suite, we'll play with a contrived package called \"toylibrary\". It's actually contained within a subdirectory of the daq-buildtools repo; however, in order to be able to build toylibrary we'll want to copy it into the base directory of the development area. Assuming you're already in the base directory, just do cp -rp daq-buildtools/toylibrary . You can now build it using the normal commands: . ./setup_build_environment ./build_daq_software.sh --install --pkgname toylibrary In terms of its actual functionality, it's pretty useless (it contains a class which can wrap an integer, and another class which can print that wrapped integer). However, its functionality is beside the point; toylibrary contains many features which DUNE DAQ packages have in common, in particular DUNE DAQ packages which provide a libraries other developers want to link against. In fact, if you run ls appfwk and ls toylibrary , you'll notice many subdirectories in common. These include: src : contains the source files meant to be built into the package's shared object library/libraries include : contains the headers users of your package should #include unittest : contains the unit tests you write to ensure that your individual classes, functions, etc. behave as expected test : contains any applications you've written for the purpose of integration testing - ensuring that your software components interact as expected If your package contains applications intended not for testing but for the end user, you'd put the code for it in a subdirectory called apps . toylibrary doesn't have this type of application, but the appfwk package does. Coding rules Along with having a standard directory structure, the C++ code itself in toylibrary conforms to the DUNE C++ Style Guide . Here, \"style\" doesn't mean whitespace and formatting, but rather, a set of Modern C++ best practices designed to make your code more robust against bugs, easier to extend, easier to reuse, etc. The DUNE C++ Style Guide is derived from the Google C++ Style Guide, but is greatly simplified and has been modified to be more appropriate to the DUNE DAQ project than Google's projects. Code which is merged into a package's git develop branch should be in conformance with the guide; while it's encouraged for code on a package's unmerged feature branches to also be in conformance, this is less important. Your project's CMakeLists.txt file Every DUNE DAQ package should have one and only one CMakeLists.txt file, in the base directory of the package (not to be confused with the base directory of the overall development area). To learn a bit about what that CMakeLists.txt file should look like, let's take a look at ./toylibrary/CMakeLists.txt . Because CMake is widely used and extensively documented online, this documentation will primarily focus on DUNE-specific CMake functions. Before doing anything else, we want to define the minimum version of CMake used (currently 3.12, which supports modern CMake style ) as well as the name and version of the project. Concerning the version: it may not literally be the case that the code you're working with is exactly the same as the version-in-question's release code, because you may be on a feature branch, or there may have been commits to the develop branch since the last release. cmake_minimum_required(VERSION 3.12) project(toylibrary VERSION 1.1.0) Next, we want to make CMake functions written specifically for DUNE DAQ development available. Near the top of the file, you'll see the following: set(CMAKE_MODULE_PATH ${CMAKE_CURRENT_SOURCE_DIR}/../daq-buildtools/cmake ${CMAKE_MODULE_PATH}) include(DAQ) This is currently (Aug-5-2020) how we ensure that the CMakeLists.txt file has access to the standard DUNE DAQ CMake functions. It imports the DAQ CMake module which is located in the daq-buildtools repo that's downloaded when you ran quick-start.sh. Note that by convention all functions/macros within the module begin with daq_ , so as to distinguish them from functions/macros from CMake modules written outside of DUNE DAQ. The next step is to call a macro from the DAQ module which sets up a standard DUNE CMake environment for your CMakeLists.txt file: daq_setup_environment() To get specific, daq_setup_environment() will do the following: * Enforce the use of standard, extension-free C++17 * Ensure libraries are built as shared, rather than static * Ensure all code within the project can find the project's public headers * Allow our linter scripts to work with the code * Tell the CMake find_package function to look in the local ./install subdirectory of the development area for packages (e.g., appfwk) * Have gcc use standard warnings * Support the use of CTest for the unit tests Next you'll see calls to CMake's find_package function, which makes toylibrary's dependencies available. Comments in the file explain why the dependencies are selected. Then, you'll see a call to a function called daq_point_build_to . CMake commands which are specific to the code in a particular subdirectory tree of your project should be clustered in the same location of the the CMakeLists.txt file, and they should be prefaced with the name of the subdirectory wrapped in the daq_point_build_to CMake function. E.g., for the code in ./toylibrary/src we have: daq_point_build_to( src ) add_library( toylibrary src/IntPrinter.cpp ) Here, daq_point_build_to(src) will ensure that the ensuing target(s) -- here, the toylibrary shared object file -- will be placed in a subdirectory of ./build/toylibrary directory (build directory) whose path mirrors the path of the source itself. To get concrete, this command means that after a successful build we'll have an executable, ./build/toylibrary/src/libtoylibrary.so , where that path is given relative to the base of your entire development area. Without this function call it would end up as ./build/toylibrary/libtoylibrary.so . With a small project such as toylibrary the benefit may not be obvious, but it becomes more so when many libraries, applications, unit tests, etc. are built within a project. Another function currently provided by the DAQ CMake module is daq_add_unit_test . Examples of this function's use can be found at the bottom of the toylibrary/CMakeLists.txt file, e.g.: daq_add_unit_test(ValueWrapper_test) If you pass this function a name, e.g., MyComponent_test , it will create a unit test executable off of a source file called <your packagename>/unittest/MyComponent_test.cxx , and handle linking in the Boost unit test dependencies. You can also optionally have it link in other libraries by providing them as arguments; in the above example, this isn't needed because ValueWrapper is a template class which is instantiated within the unit test code itself. You can see this linking, however, in ./appfwk/CMakeLists.txt . At the bottom of CMakeLists.txt, you'll see the following function: daq_install(TARGETS toylibrary) The signature of this function is hopefully self-explanatory; basically when you call it it will install the targets (executables, shared object libraries) you wish to make available to others who want to use your package in a directory called ./install/<pkgname> (here, of course, that would be ./install/toylibrary ). You'll also need to add a special file to your project for this function to work; this is discussed more fully in the \"Installing your project as a local package\" section later in this document. Some general comments about CMake While a lot can be learned from looking at the CMakeLists.txt file and reading this documentation, realize that CMake is a full-blown programming language, and like any programming language, the more familiar you are with it the easier life will be. It's extensively documented online; in particular, if you want to learn more about specific CMake functions you can go here for a reference guide of CMake commands, or, if you've sourced setup_build_environment (so CMake is set up), you can even learn at the command line via cmake --help-command <name of command> When you write code in CMake good software development principles generally apply. Writing your own macros and functions you can live up to the Don't Repeat Yourself principle ; an example of this can be found in the definition of the MakeDataTypeLibraries macro in ./appfwk/CMakeLists.txt , which helps avoid boilerplate while instantiating appfwk's FanOutDAQModule template class for various types and giving these instantiations well-motivated names. Another thing to be aware of: since you're using a single CMakeLists.txt file, the scope of your decisions can extend to all ensuing code. So, e.g., if you add the line link_libraries( bloated_library_with_extremely_common_symbol_names ) then all ensuing targets from add_executable , add_library , etc., will get that linked in. For this reason, prefer a target-specific call such as target_link_libraries(executable_that_really_needs_this_library bloated_library_with_extremely_common_symbol_names) instead. In modern CMake it's considered a best practice to avoid functions which globally link in libraries or include directories, and to make things target-specific instead. If your package relies on nonstandard dependencies As you've discovered from the Compiling-and-Running section you were pointed to at the start of this Wiki, quick-start.sh constructs a file called setup_build_environment which does what it says it's going to do. It performs ups setups of dependencies (Boost, etc.) which appfwk specifically, and in general most likely any package you'll write, rely on. If you have any new dependencies which are stored as ups products in the /cvmfs/dune.opensciencegrid.org/dunedaq/DUNE/products/ ups products area, you'll want to add them by looking for the setup_returns variable in setup_build_environment . You'll see there are several packages where for each package, there's a ups setup followed by an appending of the return value of the setup to setup_returns . You should do the same for each dependency you want to add, e.g. setup <dependency_I_am_introducing> <version_of_the_dependency> # And perhaps qualifiers, e.g., -q e19:debug setup_returns=$setup_returns\"$? \" The setup_build_environment script uses setup_returns to check that all the packages were setup without a nonzero return value. You may also want to set up additional environment variables for your new project in setup_build_environment . Installing your project as a local package Use the procedure described below in order to have your package installed. Once your package is installed, it means other packages can access the libraries, public headers, etc., provided by your package via CMake's find_package command, i.e.: # If other users call this, they can use your code find_package(mypackage) For starters, you'll want to call the DAQ module's daq_install function at the bottom of your CMakeLists.txt file. How to do so has already been described earlier in this document. You'll also want to make sure that projects which link in your code know where to find your headers. If you look in toylibrary's CMakeLists.txt file, you'll see the following: target_include_directories(toylibrary PUBLIC $<BUILD_INTERFACE:${CMAKE_CURRENT_SOURCE_DIR}/include> $<INSTALL_INTERFACE:${CMAKE_INSTALL_INCLUDEDIR}> ) ...and what this will do is tell targets in other projects where to find headers when they link in the appfwk library. The code uses CMake generator expressions so that the target will know either to look in a subdirectory of your ./install area or a local build directory depending on whether or not it's using an installed version of appfwk. ${CMAKE_INSTALL_INCLUDEDIR} is set in the GNUInstallDirs module, which is imported as a consequence of importing our DAQ module. The value of the variable defaults to \"include\" and it's unlikely you'd want to change it. A major thing you should be aware of is that when you call CMake's find_package function, it will look for a file with the name mypackageConfig.cmake in a predetermined set of directories. The daq_setup_environment function discussed at the top of this page ensures that one of those directories is your local ./install/mypackage directory. What a standard mypackageConfig.cmake file should look like with modern CMake is documented in many places on the web, but in order to make life as easy as possible there's a templatized version of this file in the daq-buildtools package. Assuming you've got a ./mypackage repo in your development area, you can do the following: cd ./mypackage cp ../daq-buildtools/configs/Config.cmake.in mypackageConfig.cmake.in and then let's look the contents of mypackageConfig.cmake.in : @PACKAGE_INIT@ include(CMakeFindDependencyMacro) # Insert find_dependency() calls for your package's dependencies in # the place of this comment. Make sure they match up with the # find_package calls in your package's CMakeLists.txt file set_and_check(targets_file ${CMAKE_CURRENT_LIST_DIR}/@PROJECT_NAME@Targets.cmake) include(${targets_file}) check_required_components(@PROJECT_NAME@) The only part of this file you need to worry about is the comment. In place of this comment, you'll want to call CMake's find_dependency function (details here ) for each package that mypackage depends on; this ensures that developers who call find_package(mypackage) don't need to have explicit find_package calls on these dependencies. You can see a simple example of this kind of file with ./toylibrary/toylibraryConfig.cmake.in , and a slightly less simple example with ./appfwk/appfwkConfig.cmake.in . Once you've edited this file as described, from the base of your development area you can then run ./build_daq_software.sh --install --pkgname mypackage without receiving an error message informing you that installation isn't an option.","title":"Creating a new package"},{"location":"versions/v1.1.0/Creating-a-new-package/#setting-up-a-development-area","text":"To create a new package, you'll want to install a DUNE-DAQ development environment and then create a new CMake project for the package. How to install and build the DUNE-DAQ development environment is described [[here|Compiling-and-running-under-v1.1.0]]. By the time you've set up the development environment as described in the linked instructions, you'll have already downloaded, built, and locally installed the CMake project for appfwk. You'll likely also have noticed that ./appfwk is a git repo; you'll want your project to reside in a git repo as well.","title":"Setting up a development area"},{"location":"versions/v1.1.0/Creating-a-new-package/#a-packages-subdirectory-structure","text":"To learn a bit more about how to structure your package so that it can be incorporated into the DUNE DAQ software suite, we'll play with a contrived package called \"toylibrary\". It's actually contained within a subdirectory of the daq-buildtools repo; however, in order to be able to build toylibrary we'll want to copy it into the base directory of the development area. Assuming you're already in the base directory, just do cp -rp daq-buildtools/toylibrary . You can now build it using the normal commands: . ./setup_build_environment ./build_daq_software.sh --install --pkgname toylibrary In terms of its actual functionality, it's pretty useless (it contains a class which can wrap an integer, and another class which can print that wrapped integer). However, its functionality is beside the point; toylibrary contains many features which DUNE DAQ packages have in common, in particular DUNE DAQ packages which provide a libraries other developers want to link against. In fact, if you run ls appfwk and ls toylibrary , you'll notice many subdirectories in common. These include: src : contains the source files meant to be built into the package's shared object library/libraries include : contains the headers users of your package should #include unittest : contains the unit tests you write to ensure that your individual classes, functions, etc. behave as expected test : contains any applications you've written for the purpose of integration testing - ensuring that your software components interact as expected If your package contains applications intended not for testing but for the end user, you'd put the code for it in a subdirectory called apps . toylibrary doesn't have this type of application, but the appfwk package does.","title":"A package's subdirectory structure"},{"location":"versions/v1.1.0/Creating-a-new-package/#coding-rules","text":"Along with having a standard directory structure, the C++ code itself in toylibrary conforms to the DUNE C++ Style Guide . Here, \"style\" doesn't mean whitespace and formatting, but rather, a set of Modern C++ best practices designed to make your code more robust against bugs, easier to extend, easier to reuse, etc. The DUNE C++ Style Guide is derived from the Google C++ Style Guide, but is greatly simplified and has been modified to be more appropriate to the DUNE DAQ project than Google's projects. Code which is merged into a package's git develop branch should be in conformance with the guide; while it's encouraged for code on a package's unmerged feature branches to also be in conformance, this is less important.","title":"Coding rules"},{"location":"versions/v1.1.0/Creating-a-new-package/#your-projects-cmakeliststxt-file","text":"Every DUNE DAQ package should have one and only one CMakeLists.txt file, in the base directory of the package (not to be confused with the base directory of the overall development area). To learn a bit about what that CMakeLists.txt file should look like, let's take a look at ./toylibrary/CMakeLists.txt . Because CMake is widely used and extensively documented online, this documentation will primarily focus on DUNE-specific CMake functions. Before doing anything else, we want to define the minimum version of CMake used (currently 3.12, which supports modern CMake style ) as well as the name and version of the project. Concerning the version: it may not literally be the case that the code you're working with is exactly the same as the version-in-question's release code, because you may be on a feature branch, or there may have been commits to the develop branch since the last release. cmake_minimum_required(VERSION 3.12) project(toylibrary VERSION 1.1.0) Next, we want to make CMake functions written specifically for DUNE DAQ development available. Near the top of the file, you'll see the following: set(CMAKE_MODULE_PATH ${CMAKE_CURRENT_SOURCE_DIR}/../daq-buildtools/cmake ${CMAKE_MODULE_PATH}) include(DAQ) This is currently (Aug-5-2020) how we ensure that the CMakeLists.txt file has access to the standard DUNE DAQ CMake functions. It imports the DAQ CMake module which is located in the daq-buildtools repo that's downloaded when you ran quick-start.sh. Note that by convention all functions/macros within the module begin with daq_ , so as to distinguish them from functions/macros from CMake modules written outside of DUNE DAQ. The next step is to call a macro from the DAQ module which sets up a standard DUNE CMake environment for your CMakeLists.txt file: daq_setup_environment() To get specific, daq_setup_environment() will do the following: * Enforce the use of standard, extension-free C++17 * Ensure libraries are built as shared, rather than static * Ensure all code within the project can find the project's public headers * Allow our linter scripts to work with the code * Tell the CMake find_package function to look in the local ./install subdirectory of the development area for packages (e.g., appfwk) * Have gcc use standard warnings * Support the use of CTest for the unit tests Next you'll see calls to CMake's find_package function, which makes toylibrary's dependencies available. Comments in the file explain why the dependencies are selected. Then, you'll see a call to a function called daq_point_build_to . CMake commands which are specific to the code in a particular subdirectory tree of your project should be clustered in the same location of the the CMakeLists.txt file, and they should be prefaced with the name of the subdirectory wrapped in the daq_point_build_to CMake function. E.g., for the code in ./toylibrary/src we have: daq_point_build_to( src ) add_library( toylibrary src/IntPrinter.cpp ) Here, daq_point_build_to(src) will ensure that the ensuing target(s) -- here, the toylibrary shared object file -- will be placed in a subdirectory of ./build/toylibrary directory (build directory) whose path mirrors the path of the source itself. To get concrete, this command means that after a successful build we'll have an executable, ./build/toylibrary/src/libtoylibrary.so , where that path is given relative to the base of your entire development area. Without this function call it would end up as ./build/toylibrary/libtoylibrary.so . With a small project such as toylibrary the benefit may not be obvious, but it becomes more so when many libraries, applications, unit tests, etc. are built within a project. Another function currently provided by the DAQ CMake module is daq_add_unit_test . Examples of this function's use can be found at the bottom of the toylibrary/CMakeLists.txt file, e.g.: daq_add_unit_test(ValueWrapper_test) If you pass this function a name, e.g., MyComponent_test , it will create a unit test executable off of a source file called <your packagename>/unittest/MyComponent_test.cxx , and handle linking in the Boost unit test dependencies. You can also optionally have it link in other libraries by providing them as arguments; in the above example, this isn't needed because ValueWrapper is a template class which is instantiated within the unit test code itself. You can see this linking, however, in ./appfwk/CMakeLists.txt . At the bottom of CMakeLists.txt, you'll see the following function: daq_install(TARGETS toylibrary) The signature of this function is hopefully self-explanatory; basically when you call it it will install the targets (executables, shared object libraries) you wish to make available to others who want to use your package in a directory called ./install/<pkgname> (here, of course, that would be ./install/toylibrary ). You'll also need to add a special file to your project for this function to work; this is discussed more fully in the \"Installing your project as a local package\" section later in this document.","title":"Your project's CMakeLists.txt file"},{"location":"versions/v1.1.0/Creating-a-new-package/#some-general-comments-about-cmake","text":"While a lot can be learned from looking at the CMakeLists.txt file and reading this documentation, realize that CMake is a full-blown programming language, and like any programming language, the more familiar you are with it the easier life will be. It's extensively documented online; in particular, if you want to learn more about specific CMake functions you can go here for a reference guide of CMake commands, or, if you've sourced setup_build_environment (so CMake is set up), you can even learn at the command line via cmake --help-command <name of command> When you write code in CMake good software development principles generally apply. Writing your own macros and functions you can live up to the Don't Repeat Yourself principle ; an example of this can be found in the definition of the MakeDataTypeLibraries macro in ./appfwk/CMakeLists.txt , which helps avoid boilerplate while instantiating appfwk's FanOutDAQModule template class for various types and giving these instantiations well-motivated names. Another thing to be aware of: since you're using a single CMakeLists.txt file, the scope of your decisions can extend to all ensuing code. So, e.g., if you add the line link_libraries( bloated_library_with_extremely_common_symbol_names ) then all ensuing targets from add_executable , add_library , etc., will get that linked in. For this reason, prefer a target-specific call such as target_link_libraries(executable_that_really_needs_this_library bloated_library_with_extremely_common_symbol_names) instead. In modern CMake it's considered a best practice to avoid functions which globally link in libraries or include directories, and to make things target-specific instead.","title":"Some general comments about CMake"},{"location":"versions/v1.1.0/Creating-a-new-package/#if-your-package-relies-on-nonstandard-dependencies","text":"As you've discovered from the Compiling-and-Running section you were pointed to at the start of this Wiki, quick-start.sh constructs a file called setup_build_environment which does what it says it's going to do. It performs ups setups of dependencies (Boost, etc.) which appfwk specifically, and in general most likely any package you'll write, rely on. If you have any new dependencies which are stored as ups products in the /cvmfs/dune.opensciencegrid.org/dunedaq/DUNE/products/ ups products area, you'll want to add them by looking for the setup_returns variable in setup_build_environment . You'll see there are several packages where for each package, there's a ups setup followed by an appending of the return value of the setup to setup_returns . You should do the same for each dependency you want to add, e.g. setup <dependency_I_am_introducing> <version_of_the_dependency> # And perhaps qualifiers, e.g., -q e19:debug setup_returns=$setup_returns\"$? \" The setup_build_environment script uses setup_returns to check that all the packages were setup without a nonzero return value. You may also want to set up additional environment variables for your new project in setup_build_environment .","title":"If your package relies on nonstandard dependencies"},{"location":"versions/v1.1.0/Creating-a-new-package/#installing-your-project-as-a-local-package","text":"Use the procedure described below in order to have your package installed. Once your package is installed, it means other packages can access the libraries, public headers, etc., provided by your package via CMake's find_package command, i.e.: # If other users call this, they can use your code find_package(mypackage) For starters, you'll want to call the DAQ module's daq_install function at the bottom of your CMakeLists.txt file. How to do so has already been described earlier in this document. You'll also want to make sure that projects which link in your code know where to find your headers. If you look in toylibrary's CMakeLists.txt file, you'll see the following: target_include_directories(toylibrary PUBLIC $<BUILD_INTERFACE:${CMAKE_CURRENT_SOURCE_DIR}/include> $<INSTALL_INTERFACE:${CMAKE_INSTALL_INCLUDEDIR}> ) ...and what this will do is tell targets in other projects where to find headers when they link in the appfwk library. The code uses CMake generator expressions so that the target will know either to look in a subdirectory of your ./install area or a local build directory depending on whether or not it's using an installed version of appfwk. ${CMAKE_INSTALL_INCLUDEDIR} is set in the GNUInstallDirs module, which is imported as a consequence of importing our DAQ module. The value of the variable defaults to \"include\" and it's unlikely you'd want to change it. A major thing you should be aware of is that when you call CMake's find_package function, it will look for a file with the name mypackageConfig.cmake in a predetermined set of directories. The daq_setup_environment function discussed at the top of this page ensures that one of those directories is your local ./install/mypackage directory. What a standard mypackageConfig.cmake file should look like with modern CMake is documented in many places on the web, but in order to make life as easy as possible there's a templatized version of this file in the daq-buildtools package. Assuming you've got a ./mypackage repo in your development area, you can do the following: cd ./mypackage cp ../daq-buildtools/configs/Config.cmake.in mypackageConfig.cmake.in and then let's look the contents of mypackageConfig.cmake.in : @PACKAGE_INIT@ include(CMakeFindDependencyMacro) # Insert find_dependency() calls for your package's dependencies in # the place of this comment. Make sure they match up with the # find_package calls in your package's CMakeLists.txt file set_and_check(targets_file ${CMAKE_CURRENT_LIST_DIR}/@PROJECT_NAME@Targets.cmake) include(${targets_file}) check_required_components(@PROJECT_NAME@) The only part of this file you need to worry about is the comment. In place of this comment, you'll want to call CMake's find_dependency function (details here ) for each package that mypackage depends on; this ensures that developers who call find_package(mypackage) don't need to have explicit find_package calls on these dependencies. You can see a simple example of this kind of file with ./toylibrary/toylibraryConfig.cmake.in , and a slightly less simple example with ./appfwk/appfwkConfig.cmake.in . Once you've edited this file as described, from the base of your development area you can then run ./build_daq_software.sh --install --pkgname mypackage without receiving an error message informing you that installation isn't an option.","title":"Installing your project as a local package"},{"location":"versions/v1.1.0/Step-by-step-instructions-for-creating-your-own-DAQModule-package/","text":"Introduction This page is intended to provide a step-by-step guide to setting up a work area in which you create software modules that run within the DUNE DAQ software application framework ( appfwk ). These modules are often called DAQModules since that is the C++ base class that they inherit from. If you are impatient to get started, you can jump to the [[Step-by-step Instructions|Step-by-step-instructions-for-creating-your-own-DAQModule-package-under-v1.1.0#step-by-step Installation instructions]] now. New users should probably continue reading this introduction, though, to learn a little bit of background information. This page draws from, and references, several other Wiki pages in the appfwk repository. Links to these pages are listed in the text in the relevant places, and they are a good source of additional information for interested readers. Here are some of these references collected for you in one place on this page: * [[Compiling and running the App Fwk|Compiling-and-running-under-v1.1.0]] * [[Creating a new package|Creating-a-new-package-under-v1.1.0]] and last, but certainly not least * The DUNE DAQ C++ Style Guide This page assumes a basic understanding of how modules are combined to form processes within appfwk v1. A brief recap can be found in the following snippet. A reference for this is the sneak peak talk given by Eric Flumerfelt on 15-Jun-2020. As a refresher: * a *DAQProcess* contains one or more *DAQModules* * when multiple *DAQModules* are present within a *DAQProcess*, they communicate with each other via *Queues* (the diagrams that are included [[later on this page|Step-by-step-instructions-for-creating-your-own-DAQModule-package-under-v1#information-about-the-daqmodules-in-the-listrev-example]] might help explain this) * there are two classes that wrap *Queues* to provide the ability to push data onto the queue (*DAQSink*) or pull data from a queue (*DAQSource*). An individual *DAQModule* will only access one side of each *Queue*. If the module pushes data onto the queue, it will use an instance of the *DAQSink* class (which wraps the desired *Queue*), and if the module pops data from the queue, it will use an instance of the *DAQSource* class. * the *DAQModules* that run within a given process (and the *Queues* between them) are specified in a JSON *process configuration* file. An example of one such file is given below. The creation of the *DAQModules* and *Queues* is handled by the *appfwk*. * at this point in time, we expect that most users will be developing *DAQModules* and simply using existing *Queues* from the *appfwk*. * at the moment, there aren't any centrally-provided libraries, tools, or recommendations for inter-process communication. We expect to address this topic soon, but for now, developers can either focus on single-process examples, or use other software for inter-process communication. The instructions on this page focus on single-process examples. * please remember that there have been a number of compromises or simplifications in the functionality that is provided by *appfwk* v1. We have made a good faith attempt to provide a good start on the internal interfaces and layout of *DAQProcesses*, but things will definitely change over time, and we will be gathering feedback from everyone on ways that things might be improved. (Of course, we already have a fairly good list based on the experience of creating v1.) Step-by-step Installation instructions Instructions on setting up a work area in which you can create your new package can be found [[here|Compiling-and-running-under-v1.1.0]]. After you've gone through those instructions, for our purposes the first build command we'll want to run will one which installs appfwk in a local ./install subdirectory. * ./build_daq_software.sh --install You can then link your own code against your local installation of appfwk. * If you would like to try running the example application(s) from the core appfwk repository, you can do that now. There are instructions [[later on this page|Step-by-step-instructions-for-creating-your-own-DAQModule-package-under-v1.1.0#running-the-fanout-example-in-the-appfwk-repo]] for doing that. At this point, you could either check out the example DAQModule package, build it, and run the example application (recommended), or you could jump to [[creating your own package|Step-by-step-instructions-for-creating-your-own-DAQModule-package-under-v1.1.0#Create-your-own-software-package]]. These instructions will walk you through doing both, but of course you can skip to the latter by scrolling to the dedicated section. Install the example package Here are the steps for adding the appfwk Example package to your work area, building it, and running the example application... (For information on the DAQModules that are contained in the example application and how they interact, please see [[this section|Step-by-step-instructions-for-creating-your-own-DAQModule-package-under-v1.1.0#Information-about-the-DAQModules-in-the-listrev-example]] later on this page.) * cd into your work area directory ( WORK_DIR ), if you aren't there already * run the following command to clone the example package: * git clone https://github.com/DUNE-DAQ/listrev.git * Make sure you're using v1.1.0 * cd listrev; git checkout v1.1.0; cd .. * [if not already done in the current shell] set up the build environment for your work area * source ./setup_build_environment * Build the listrev in your work area by running * ./build_daq_software.sh --pkgname listrev In order to run the example package follow the instruction in the [[dedicated section|Step-by-step-instructions-for-creating-your-own-DAQModule-package-under-v1.1.0#Running the example package]]. Create your own software package Here are the commands to create your own software package that depends on appfwk: * create a new directory underneath WORK_DIR (we'll call this YOUR_PKG_DIR ) * copy the CMakeLists.txt file from the example package into YOUR_PKG_DIR (you can fetch it from here ) * edit this CMakeLists.txt file to change all of the instances of the string \"listrev\" to your package name * (we'll do a couple more edits in a bit, but you can save and close this file now * create a src directory underneath YOUR_PKG_DIR * copy one of the DAQModules from the example package into the src directory * as an example, let's copy the RandomDataListGenerator DAQModule from the example package, along with the CommonIssues header file. You can fetch these files from here , here , and here . * change all instances of \"listrev\" and \"LISTREV\" in these files to your package name * edit YOUR_PKG_DIR /CMakeLists.txt to remove the ListReverser and ReversedListValidator add_library and target_link_libraries lines * you should also comment out the daq_point_build_to( test ) , file(COPY test/list_reversal_app.json DESTINATION test) and daq_install(... lines * cd to WORK_DIR * [if not already done in the current shell] set up the build environment for your work area * source ./setup_build_environment * rebuild your package in your work area by running * ./build_daq_software.sh --pkgname <your package name> * at this point, you'll have a decent start on your own DAQModule package. Of course, you'll need to rename and modify the RandomDataListGenerator DAQModule to do whatever you want your first DAQModule to do. And, when you get the point of running your DAQProcess that uses your DAQModule (s), you'll need to create a JSON process configuration file to use for that, but hopefully the listrev example package will help. How to write a DAQModule From the C++ point of view, DAQ modules are implementation of a [[DAQ Module interface|DAQModules]]. That means that apart from the constructor (that receives a name) only one method has to be implemented: init() . The function has to handle all those configuration items that are not supposed to be reconfigurable during the run. Optionally it can configure all those variables that can be overridden during run time. The most important thing that init has to do is to call register_command in order to associate functions to each command that the module is expected to support. Each function has to be a set of operations that changes the internal status of the module according to the command that it is implementing. The prototype of each function is in the form void do_something( const vector<string> & args ) ; The function can be virtual and the system is able to pick the most downstream specification. Assuming that the name of the module you are developing is MyNewDAQModule a call to associate the command \"start\" to a function called do_start will be register_command(\"start\", & MyNewDAQModule::do_start); It is expected that the operations of the DAQ Module are carried on in other threads under the control of the DAQModule itself. How to make your DAQModule a plugin of the appfmw DAQModules need to be created with a factory pattern simply via their name. In the application framework this is done using CETlib . In order to do that, each implementation ( cpp file) needs to have a call to the macro DEFINE_DUNE_DAQ_MODULE(<name_including_namespace>) for example DEFINE_DUNE_DAQ_MODULE(dunedaq::appfwk::DummyModule) In order to generate a shared object library that is linkable in runtime from the application framework, the name of the library has to be the in the form <package>_<module_name_no_namespace>_duneDAQModule . This can be achieved simply adding a line in the CMakeLists.txt file of your project in the form add_library(<package>_<module_name_no_namespace>_duneDAQModule path/to/my/file.cpp) . For example: add_library(appfwk_DummyModule_duneDAQModule test/DummyModule.cpp) Running examples Running the example package Assuming you installed your software as described in the previous sections, these are the instructions to run the code: * set up the runtime environment setup script by running this command: * source ./setup_runtime_environment * please note that this needs to be done from WORK_DIR * run the example application using the following command: * daq_application -c QueryResponseCommandFacility -j build/listrev/test/list_reversal_app.json * once the program is running, and you see the \"Enter a command\" prompt, you can type in commands like the following: * configure * This command pretends to set the values of configurable parameters like the number of integers in each randomly generated list * start * This command is passed to the three DAQModules in a well-specified order (specified in the JSON process configuration file). First, the validator is started, then the reverser , then the generator . Once the generator is started, it begins creating lists of integers and passing them to the other two DAQModules . * On the console, you will see ERS LOG messages from each of the DAQModules saying that they started (so you can confirm that the start order is correct), and then you will see ERS DEBUG messages that tell you what each of the DAQModules is doing as they process the lists of integers. * stop * This command stops the three DAQModules , in the reverse order that they were started (like the start order, the stop order is specified in the JSON process config file). * You will need to type this command into the console a little blindly, since the ERS DEBUG messages will be printing to the console as the program runs. * After each of the three DAQModules has finished, they print an ERS INFO message to the console with a summary of what they accomplished. * unconfigure * This command pretends to tear-down whatever configuration was established in the configure step. * quit * This command exits the program. Information about the DAQModules in the listrev example The idea behind the listrev example is to have one DAQModule that generates a list of random integers, another DAQModule that reverses a copy of the list, and a third DAQModule that compares copies of the original and reversed lists to validate that they are equivalent, modulo the reversal. This is shown in the following diagrams. The first one provides a little description of what each of the DAQModules is doing, and the second one shows the class names of the DAQModules and the configured names of the DAQModules and the Queues , as they are identified in the JSON process configuration file. A copy of the listrev JSON process configuration file is shown below (the official copy of this file in the repo is here ). As you can see, the Queues are specified first, then the DAQModules , and the DAQModules include the configuration of which Queues they make use of. * The capacity and kind parameters within the queue declarations are required. The names of the queues (e.g. primaryDataQueue ) are your choice. * The user_module_type parameter in the module declarations is required. The names of the parameters that specify the queues to the modules are up to you. As you can see in this example, different parameter names are used to specify the queues to the three different modules. (Of course, what the DAQModule code does with the queue names is standardized. An example of that is shown in this snippet of listrev code .) { \"queues\": { \"primaryDataQueue\": { \"capacity\": 10, \"kind\": \"FollySPSCQueue\" }, \"reversedDataQueue\": { \"capacity\": 10, \"kind\": \"FollySPSCQueue\" }, \"dataCopyQueue\": { \"capacity\": 10, \"kind\": \"FollySPSCQueue\" } }, \"modules\": { \"generator\": { \"user_module_type\": \"RandomDataListGenerator\", \"outputs\": [ \"primaryDataQueue\", \"dataCopyQueue\" ] }, \"reverser\": { \"user_module_type\": \"ListReverser\", \"input\": \"primaryDataQueue\", \"output\": \"reversedDataQueue\" }, \"validator\": { \"user_module_type\": \"ReversedListValidator\", \"reversed_data_input\": \"reversedDataQueue\", \"original_data_input\": \"dataCopyQueue\" } }, \"commands\": { \"start\": [ \"validator\", \"reverser\", \"generator\" ], \"stop\": [ \"generator\", \"reverser\", \"validator\" ] } } As mentioned earlier on this page, when you run the listrev example, you will see ERS messages printed out in the console. ERS is the Error Reporting Service from the ATLAS experiment. Within the DUNE DAQ, we have our own fork of that package, and you will see a clone of the DUNE DAQ ERS repo when you look at the directories underneath WORK_DIR . For further information on ERS, you can see this journal article (PDF) . In addition to ERS messages, there are TRACE messages in the listrev example code. The current model for using ERS and TRACE is described in a section of the Style Guide , and the example code follows those guidelines. For example, the example DAQModule s usee ERS Issues for warnings, errors, and fatal errors, and TRACE messages for messages that developers would use for debugging or verifying the behavior of the code. The periodic progress reports are implemented as ERS DEBUG messages, although admittedly, they could just as easily been implemented as TRACE messages. The choice there was simply a practical one - that ERS messages are displayed on the console by default, whereas TRACE messages typically go to a memory area by default so that they use very little system resources. (TRACE messages can trivially be directed to the console, but that would have been just one more step in the instructions above.) Users who are interested in seeing the TRACE messages from the listrev example code, or their own DAQModules when the time comes, can use the following steps: * before running the program, set the TRACE_FILE environmental variable to point to a file underneath your WORK_DIR * export TRACE_FILE=<WORK_DIR>/log/${USER}_dunedaq.trace * run the program * look at the TRACE levels that are enabled for each TRACE_NAME (TRACE_NAMEs are used to help identify which source file the messages were sent from) * tlvls * enable the TRACE levels that you would like to see appear in the TRACE memory buffer with commands like the following: * tonM -n RandomDataListGenerator 10 * tonM -n RandomDataListGenerator 15 * view the messages in the TRACE memory buffer. I appreciate seeing the timestamps in human-readable form, so I typically pipe the output of tshow to tdelta as shown here (both are provided by the TRACE package) * tshow | tdelta -ct 1 | more * Note that the messages from the TRACE buffer are displayed in reverse time order (most recent message first) This short introduction to TRACE only describes a small fraction of its capabilities, and interested users are encouraged to read the Quick Start guide, the User's Guide, and other documentation provided here . Running the Fanout example in the appfwk repo In a fresh shell, here are the steps that you would use to run the Fanout example in the appfwk repo after you have run quick-start.sh and built the software using build_daq_software.sh : * cd to your WORK_DIR * run the following commands to set up the build and runtime environments: * source ./setup_build_environment * source ./setup_runtime_environment * run the following command to start the example: * build/appfwk/apps/daq_application -c QueryResponseCommandFacility -j appfwk/test/producer_consumer_dynamic_test.json * enter commands like the following: * configure * start * stop * quit","title":"Introduction"},{"location":"versions/v1.1.0/Step-by-step-instructions-for-creating-your-own-DAQModule-package/#introduction","text":"This page is intended to provide a step-by-step guide to setting up a work area in which you create software modules that run within the DUNE DAQ software application framework ( appfwk ). These modules are often called DAQModules since that is the C++ base class that they inherit from. If you are impatient to get started, you can jump to the [[Step-by-step Instructions|Step-by-step-instructions-for-creating-your-own-DAQModule-package-under-v1.1.0#step-by-step Installation instructions]] now. New users should probably continue reading this introduction, though, to learn a little bit of background information. This page draws from, and references, several other Wiki pages in the appfwk repository. Links to these pages are listed in the text in the relevant places, and they are a good source of additional information for interested readers. Here are some of these references collected for you in one place on this page: * [[Compiling and running the App Fwk|Compiling-and-running-under-v1.1.0]] * [[Creating a new package|Creating-a-new-package-under-v1.1.0]] and last, but certainly not least * The DUNE DAQ C++ Style Guide This page assumes a basic understanding of how modules are combined to form processes within appfwk v1. A brief recap can be found in the following snippet. A reference for this is the sneak peak talk given by Eric Flumerfelt on 15-Jun-2020. As a refresher: * a *DAQProcess* contains one or more *DAQModules* * when multiple *DAQModules* are present within a *DAQProcess*, they communicate with each other via *Queues* (the diagrams that are included [[later on this page|Step-by-step-instructions-for-creating-your-own-DAQModule-package-under-v1#information-about-the-daqmodules-in-the-listrev-example]] might help explain this) * there are two classes that wrap *Queues* to provide the ability to push data onto the queue (*DAQSink*) or pull data from a queue (*DAQSource*). An individual *DAQModule* will only access one side of each *Queue*. If the module pushes data onto the queue, it will use an instance of the *DAQSink* class (which wraps the desired *Queue*), and if the module pops data from the queue, it will use an instance of the *DAQSource* class. * the *DAQModules* that run within a given process (and the *Queues* between them) are specified in a JSON *process configuration* file. An example of one such file is given below. The creation of the *DAQModules* and *Queues* is handled by the *appfwk*. * at this point in time, we expect that most users will be developing *DAQModules* and simply using existing *Queues* from the *appfwk*. * at the moment, there aren't any centrally-provided libraries, tools, or recommendations for inter-process communication. We expect to address this topic soon, but for now, developers can either focus on single-process examples, or use other software for inter-process communication. The instructions on this page focus on single-process examples. * please remember that there have been a number of compromises or simplifications in the functionality that is provided by *appfwk* v1. We have made a good faith attempt to provide a good start on the internal interfaces and layout of *DAQProcesses*, but things will definitely change over time, and we will be gathering feedback from everyone on ways that things might be improved. (Of course, we already have a fairly good list based on the experience of creating v1.)","title":"Introduction"},{"location":"versions/v1.1.0/Step-by-step-instructions-for-creating-your-own-DAQModule-package/#step-by-step-installation-instructions","text":"Instructions on setting up a work area in which you can create your new package can be found [[here|Compiling-and-running-under-v1.1.0]]. After you've gone through those instructions, for our purposes the first build command we'll want to run will one which installs appfwk in a local ./install subdirectory. * ./build_daq_software.sh --install You can then link your own code against your local installation of appfwk. * If you would like to try running the example application(s) from the core appfwk repository, you can do that now. There are instructions [[later on this page|Step-by-step-instructions-for-creating-your-own-DAQModule-package-under-v1.1.0#running-the-fanout-example-in-the-appfwk-repo]] for doing that. At this point, you could either check out the example DAQModule package, build it, and run the example application (recommended), or you could jump to [[creating your own package|Step-by-step-instructions-for-creating-your-own-DAQModule-package-under-v1.1.0#Create-your-own-software-package]]. These instructions will walk you through doing both, but of course you can skip to the latter by scrolling to the dedicated section.","title":"Step-by-step Installation instructions"},{"location":"versions/v1.1.0/Step-by-step-instructions-for-creating-your-own-DAQModule-package/#install-the-example-package","text":"Here are the steps for adding the appfwk Example package to your work area, building it, and running the example application... (For information on the DAQModules that are contained in the example application and how they interact, please see [[this section|Step-by-step-instructions-for-creating-your-own-DAQModule-package-under-v1.1.0#Information-about-the-DAQModules-in-the-listrev-example]] later on this page.) * cd into your work area directory ( WORK_DIR ), if you aren't there already * run the following command to clone the example package: * git clone https://github.com/DUNE-DAQ/listrev.git * Make sure you're using v1.1.0 * cd listrev; git checkout v1.1.0; cd .. * [if not already done in the current shell] set up the build environment for your work area * source ./setup_build_environment * Build the listrev in your work area by running * ./build_daq_software.sh --pkgname listrev In order to run the example package follow the instruction in the [[dedicated section|Step-by-step-instructions-for-creating-your-own-DAQModule-package-under-v1.1.0#Running the example package]].","title":"Install the example package"},{"location":"versions/v1.1.0/Step-by-step-instructions-for-creating-your-own-DAQModule-package/#create-your-own-software-package","text":"Here are the commands to create your own software package that depends on appfwk: * create a new directory underneath WORK_DIR (we'll call this YOUR_PKG_DIR ) * copy the CMakeLists.txt file from the example package into YOUR_PKG_DIR (you can fetch it from here ) * edit this CMakeLists.txt file to change all of the instances of the string \"listrev\" to your package name * (we'll do a couple more edits in a bit, but you can save and close this file now * create a src directory underneath YOUR_PKG_DIR * copy one of the DAQModules from the example package into the src directory * as an example, let's copy the RandomDataListGenerator DAQModule from the example package, along with the CommonIssues header file. You can fetch these files from here , here , and here . * change all instances of \"listrev\" and \"LISTREV\" in these files to your package name * edit YOUR_PKG_DIR /CMakeLists.txt to remove the ListReverser and ReversedListValidator add_library and target_link_libraries lines * you should also comment out the daq_point_build_to( test ) , file(COPY test/list_reversal_app.json DESTINATION test) and daq_install(... lines * cd to WORK_DIR * [if not already done in the current shell] set up the build environment for your work area * source ./setup_build_environment * rebuild your package in your work area by running * ./build_daq_software.sh --pkgname <your package name> * at this point, you'll have a decent start on your own DAQModule package. Of course, you'll need to rename and modify the RandomDataListGenerator DAQModule to do whatever you want your first DAQModule to do. And, when you get the point of running your DAQProcess that uses your DAQModule (s), you'll need to create a JSON process configuration file to use for that, but hopefully the listrev example package will help.","title":"Create your own software package"},{"location":"versions/v1.1.0/Step-by-step-instructions-for-creating-your-own-DAQModule-package/#how-to-write-a-daqmodule","text":"From the C++ point of view, DAQ modules are implementation of a [[DAQ Module interface|DAQModules]]. That means that apart from the constructor (that receives a name) only one method has to be implemented: init() . The function has to handle all those configuration items that are not supposed to be reconfigurable during the run. Optionally it can configure all those variables that can be overridden during run time. The most important thing that init has to do is to call register_command in order to associate functions to each command that the module is expected to support. Each function has to be a set of operations that changes the internal status of the module according to the command that it is implementing. The prototype of each function is in the form void do_something( const vector<string> & args ) ; The function can be virtual and the system is able to pick the most downstream specification. Assuming that the name of the module you are developing is MyNewDAQModule a call to associate the command \"start\" to a function called do_start will be register_command(\"start\", & MyNewDAQModule::do_start); It is expected that the operations of the DAQ Module are carried on in other threads under the control of the DAQModule itself.","title":"How to write a DAQModule"},{"location":"versions/v1.1.0/Step-by-step-instructions-for-creating-your-own-DAQModule-package/#how-to-make-your-daqmodule-a-plugin-of-the-appfmw","text":"DAQModules need to be created with a factory pattern simply via their name. In the application framework this is done using CETlib . In order to do that, each implementation ( cpp file) needs to have a call to the macro DEFINE_DUNE_DAQ_MODULE(<name_including_namespace>) for example DEFINE_DUNE_DAQ_MODULE(dunedaq::appfwk::DummyModule) In order to generate a shared object library that is linkable in runtime from the application framework, the name of the library has to be the in the form <package>_<module_name_no_namespace>_duneDAQModule . This can be achieved simply adding a line in the CMakeLists.txt file of your project in the form add_library(<package>_<module_name_no_namespace>_duneDAQModule path/to/my/file.cpp) . For example: add_library(appfwk_DummyModule_duneDAQModule test/DummyModule.cpp)","title":"How to make your DAQModule a plugin of the appfmw"},{"location":"versions/v1.1.0/Step-by-step-instructions-for-creating-your-own-DAQModule-package/#running-examples","text":"","title":"Running examples"},{"location":"versions/v1.1.0/Step-by-step-instructions-for-creating-your-own-DAQModule-package/#running-the-example-package","text":"Assuming you installed your software as described in the previous sections, these are the instructions to run the code: * set up the runtime environment setup script by running this command: * source ./setup_runtime_environment * please note that this needs to be done from WORK_DIR * run the example application using the following command: * daq_application -c QueryResponseCommandFacility -j build/listrev/test/list_reversal_app.json * once the program is running, and you see the \"Enter a command\" prompt, you can type in commands like the following: * configure * This command pretends to set the values of configurable parameters like the number of integers in each randomly generated list * start * This command is passed to the three DAQModules in a well-specified order (specified in the JSON process configuration file). First, the validator is started, then the reverser , then the generator . Once the generator is started, it begins creating lists of integers and passing them to the other two DAQModules . * On the console, you will see ERS LOG messages from each of the DAQModules saying that they started (so you can confirm that the start order is correct), and then you will see ERS DEBUG messages that tell you what each of the DAQModules is doing as they process the lists of integers. * stop * This command stops the three DAQModules , in the reverse order that they were started (like the start order, the stop order is specified in the JSON process config file). * You will need to type this command into the console a little blindly, since the ERS DEBUG messages will be printing to the console as the program runs. * After each of the three DAQModules has finished, they print an ERS INFO message to the console with a summary of what they accomplished. * unconfigure * This command pretends to tear-down whatever configuration was established in the configure step. * quit * This command exits the program.","title":"Running the example package"},{"location":"versions/v1.1.0/Step-by-step-instructions-for-creating-your-own-DAQModule-package/#information-about-the-daqmodules-in-the-listrev-example","text":"The idea behind the listrev example is to have one DAQModule that generates a list of random integers, another DAQModule that reverses a copy of the list, and a third DAQModule that compares copies of the original and reversed lists to validate that they are equivalent, modulo the reversal. This is shown in the following diagrams. The first one provides a little description of what each of the DAQModules is doing, and the second one shows the class names of the DAQModules and the configured names of the DAQModules and the Queues , as they are identified in the JSON process configuration file. A copy of the listrev JSON process configuration file is shown below (the official copy of this file in the repo is here ). As you can see, the Queues are specified first, then the DAQModules , and the DAQModules include the configuration of which Queues they make use of. * The capacity and kind parameters within the queue declarations are required. The names of the queues (e.g. primaryDataQueue ) are your choice. * The user_module_type parameter in the module declarations is required. The names of the parameters that specify the queues to the modules are up to you. As you can see in this example, different parameter names are used to specify the queues to the three different modules. (Of course, what the DAQModule code does with the queue names is standardized. An example of that is shown in this snippet of listrev code .) { \"queues\": { \"primaryDataQueue\": { \"capacity\": 10, \"kind\": \"FollySPSCQueue\" }, \"reversedDataQueue\": { \"capacity\": 10, \"kind\": \"FollySPSCQueue\" }, \"dataCopyQueue\": { \"capacity\": 10, \"kind\": \"FollySPSCQueue\" } }, \"modules\": { \"generator\": { \"user_module_type\": \"RandomDataListGenerator\", \"outputs\": [ \"primaryDataQueue\", \"dataCopyQueue\" ] }, \"reverser\": { \"user_module_type\": \"ListReverser\", \"input\": \"primaryDataQueue\", \"output\": \"reversedDataQueue\" }, \"validator\": { \"user_module_type\": \"ReversedListValidator\", \"reversed_data_input\": \"reversedDataQueue\", \"original_data_input\": \"dataCopyQueue\" } }, \"commands\": { \"start\": [ \"validator\", \"reverser\", \"generator\" ], \"stop\": [ \"generator\", \"reverser\", \"validator\" ] } } As mentioned earlier on this page, when you run the listrev example, you will see ERS messages printed out in the console. ERS is the Error Reporting Service from the ATLAS experiment. Within the DUNE DAQ, we have our own fork of that package, and you will see a clone of the DUNE DAQ ERS repo when you look at the directories underneath WORK_DIR . For further information on ERS, you can see this journal article (PDF) . In addition to ERS messages, there are TRACE messages in the listrev example code. The current model for using ERS and TRACE is described in a section of the Style Guide , and the example code follows those guidelines. For example, the example DAQModule s usee ERS Issues for warnings, errors, and fatal errors, and TRACE messages for messages that developers would use for debugging or verifying the behavior of the code. The periodic progress reports are implemented as ERS DEBUG messages, although admittedly, they could just as easily been implemented as TRACE messages. The choice there was simply a practical one - that ERS messages are displayed on the console by default, whereas TRACE messages typically go to a memory area by default so that they use very little system resources. (TRACE messages can trivially be directed to the console, but that would have been just one more step in the instructions above.) Users who are interested in seeing the TRACE messages from the listrev example code, or their own DAQModules when the time comes, can use the following steps: * before running the program, set the TRACE_FILE environmental variable to point to a file underneath your WORK_DIR * export TRACE_FILE=<WORK_DIR>/log/${USER}_dunedaq.trace * run the program * look at the TRACE levels that are enabled for each TRACE_NAME (TRACE_NAMEs are used to help identify which source file the messages were sent from) * tlvls * enable the TRACE levels that you would like to see appear in the TRACE memory buffer with commands like the following: * tonM -n RandomDataListGenerator 10 * tonM -n RandomDataListGenerator 15 * view the messages in the TRACE memory buffer. I appreciate seeing the timestamps in human-readable form, so I typically pipe the output of tshow to tdelta as shown here (both are provided by the TRACE package) * tshow | tdelta -ct 1 | more * Note that the messages from the TRACE buffer are displayed in reverse time order (most recent message first) This short introduction to TRACE only describes a small fraction of its capabilities, and interested users are encouraged to read the Quick Start guide, the User's Guide, and other documentation provided here .","title":"Information about the DAQModules in the listrev example"},{"location":"versions/v1.1.0/Step-by-step-instructions-for-creating-your-own-DAQModule-package/#running-the-fanout-example-in-the-appfwk-repo","text":"In a fresh shell, here are the steps that you would use to run the Fanout example in the appfwk repo after you have run quick-start.sh and built the software using build_daq_software.sh : * cd to your WORK_DIR * run the following commands to set up the build and runtime environments: * source ./setup_build_environment * source ./setup_runtime_environment * run the following command to start the example: * build/appfwk/apps/daq_application -c QueryResponseCommandFacility -j appfwk/test/producer_consumer_dynamic_test.json * enter commands like the following: * configure * start * stop * quit","title":"Running the Fanout example in the appfwk repo"},{"location":"versions/v1.1.0/Testing/","text":"Broadly speaking, there are two types of test program you can run: unit tests, and integration tests. Unit tests Unit tests are standalone programs with names of the form <code unit under test>_test and are implemented in the ./unittest subdirectory of a package. They're written using the Boost.Test library. For complete documentation on the Boost.Test library, you can look at the Boost documentation ; for substantially quicker but still useful intros, take a look here and here . While you can run a unit test program without arguments, some arguments are very useful - in particular, those that control which test cases are run, and those that control the level of logging output. To maximize the output to screen, add the argument -l all , and to minimize it, add -l nothing . Other intermediate levels are covered in the Boost documentation. To skip a test, you can add an argument of the form --run_test=\\!test_you_dont_want ; note the exclamation point needs to be escaped with a backslash since the shell interprets it as a special character otherwise. Integration tests Integration tests look at how various units of code interact; unlike unit tests, there's no common approach to running them. As such, a list of integration tests and how to run them can be found in the list below: [[Queue testing|Queue-testing]] dummy_test_app: A simple application that statically instantiates a DummyModule DAQModule, which simply takes the command \"stuff\" to echo a string producer_consumer_dynamic_test.json: A sample configuration file for daq_application which runs a version of the [[Simple readout application|Simple-readout-application]]","title":"Testing"},{"location":"versions/v1.1.0/Testing/#unit-tests","text":"Unit tests are standalone programs with names of the form <code unit under test>_test and are implemented in the ./unittest subdirectory of a package. They're written using the Boost.Test library. For complete documentation on the Boost.Test library, you can look at the Boost documentation ; for substantially quicker but still useful intros, take a look here and here . While you can run a unit test program without arguments, some arguments are very useful - in particular, those that control which test cases are run, and those that control the level of logging output. To maximize the output to screen, add the argument -l all , and to minimize it, add -l nothing . Other intermediate levels are covered in the Boost documentation. To skip a test, you can add an argument of the form --run_test=\\!test_you_dont_want ; note the exclamation point needs to be escaped with a backslash since the shell interprets it as a special character otherwise.","title":"Unit tests"},{"location":"versions/v1.1.0/Testing/#integration-tests","text":"Integration tests look at how various units of code interact; unlike unit tests, there's no common approach to running them. As such, a list of integration tests and how to run them can be found in the list below: [[Queue testing|Queue-testing]] dummy_test_app: A simple application that statically instantiates a DummyModule DAQModule, which simply takes the command \"stuff\" to echo a string producer_consumer_dynamic_test.json: A sample configuration file for daq_application which runs a version of the [[Simple readout application|Simple-readout-application]]","title":"Integration tests"}]}